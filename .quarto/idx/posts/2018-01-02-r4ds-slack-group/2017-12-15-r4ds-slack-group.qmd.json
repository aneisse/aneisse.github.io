{"title":"**R4DS Slack Community data**","markdown":{"yaml":{"title":"**R4DS Slack Community data**","description":"Data analysis of signup data from R4DS","author":"Anderson Neisse","date":"2017-12-15","categories":["Statistics","Data Science","R4DS","Maps","Exploratory Analysis"],"image":"featured.png","toc":true},"headingText":"1 The group","containsRefs":false,"markdown":"\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(fig.align = 'center')\n```\n\nBefore anything else, I tried HARD to make tabsets work with `blogdown` but it doesn't offer support to tabsets at all, nor folded code chunks, sadly. That said, this post might look longer than it really is! Unless you want to read every chunk of code, then it indeed IS as long as it seems.\n\n\nThe [R for Data Science](http://r4ds.had.co.nz/) book (R4DS) is a wonderfull book to introduce people both in the world of Data Science and R/RStudio. But something that improves the reading-through experience is the **R4DS Online Learning Community**. A Slack community that basically provides the means for people discuss about the book and anything that comes from that, like: sharing data sources, additional learning material, and helping each other to read it through. \n\nI joined it in the beginning, when Jesse Maegan [\\@kierisi](https://twitter.com/kierisi) invited people on Twitter. The story since then has been told in [her blog on Medium](https://medium.com/@kierisi). Now the community have had some improvements in structure (aaand some acquaintances in administration) so there can be weekly invitations. We're on the first round since these changes (the second since the beginning), if you have interest in the community don't be shy, join us! You can apply [HERE](https://docs.google.com/forms/d/e/1FAIpQLSeT3zfzjWxoaQ6RmUEdT9n0xtvkuSaMeBetDQLpzNJvGUB6IQ/viewform).\n\nAnd that's what this post is about, some visualizations on the signup data! We have some variables about our members that might render cool graphs and a members' world map! But first we'll have to download that data and we're going to do that from inside R! Let's dive in!\n\n# 2 The data and `googlesheets`\n\nThe group's admins maintain a google sheet with unidentified data on all the members with a few variables. This post will show some visualizations of this data. However, since it's maintained in Google Drive we'll first go through a quick tutorial on the `googlesheets` package to load worksheets directly from Google Drive to R(Studio).\n\nThat said, the packages we'll use in this post are the ones listed below:\n\n```{r, message=FALSE, warning=FALSE, results='hide'}\n#Used packages\nlibrary(tidyverse) #Data munging and vizualizing\nlibrary(googlesheets4) #Loading google sheets into R\nlibrary(plotly) #Plotting interactive graphs\nlibrary(shiny) #Package for R-based web apps, I use it for the dev() function to align plots produced with plotly\nlibrary(leaflet) #Ploting interactive maps\nlibrary(ggmap) #Package for google maps. Used here to obtain coordinates based on location\n```\n\nNow, `googlesheets` uses your Google Drive account to access the worksheets that you have access to and then work from that. So the first thing to do is share the data source into your Google Drive account. The googlesheet can be accessed here: [R4DS deidentified data](https://docs.google.com/spreadsheets/d/1L7lU5QqVfUtZXenFsGErPFS4n1jL4fixx9d3lrF94hU/edit#gid=0).\n\nWith the sheet in your account now it is possible to access it with `googlesheets`. The function `googlesheets::gs_ls()` will list all the sheets that you have access to. \n\nIt is worth to note that if it is the first time you are running `gs_ls()` you might have to authenticate your Google Drive Account and allow `googlesheets` to get data from your account.\n\n```{r eval=FALSE, warning = FALSE}\n#Listing the accessible google sheets\ngs_ls()\n```\n\nFrom the `gs_ls()` command you might have figured that the first column is the sheet's name (title). The title will be required for us to download the data. For the data the titl is \"R4DS deidentified data\n\nThe package `googlesheets` perform API calls to Google Drive to obtain the data from the specified sheet. Any API call depends on a set of parameters that, in this case, `googlesheets` will handle for us. The `gs_title()` function obtains the API parameters based on the worksheet's title, which we will provide.\n\n```{r, eval=FALSE, warning = FALSE}\n#Getting the API parameters for the desired google sheet\nsheet_reference <- gs_title(\"R4DS deidentified data\", verbose = F)\n```\n\nThe parameters were assigned to the `sheet_reference` variable. If everything went alright you will see a \"Sheet successfully identified: 'R4DS deidentified data'\" message in the prompt. Now, if you run `sheet_reference` you can see all the parameters.\n\n\nNow that you specified which google sheet you want, you may also list the sheets inside a worksheet with `gs_ws_ls()`. When the worksheet you are interested in has more than one sheet, this command will be useful for you to specify exactly which sheet you want to download. if not specified, by default the package will download the first sheet in the worksheet.\n\n```{r, eval=FALSE, warning = FALSE}\n#Listing all the sheets in our desired google sheet\ngs_ws_ls(sheet_reference)\n```\n\nFor the \"R4DS unidentified data\" google sheet there's just one sheet named \"Sheet1\". Since the package downloads the first sheet by default, there is no need for us to specify it.\n\nParameters at hand! Now we can download and have a first look at the data with `googlesheets::gs_read()` and assing it to the `dados` variable. The word \"dados\" is brazilian for data, thats my way of broadly naming the data while preventing conflicts with any function's \"data\" parameter. Consider it my nationality's contribution for this post! ;D\n\n```{r, eval=FALSE, message=FALSE, warning = FALSE}\n#Downloading the data from the google sheet\n(dados <- gs_read(ss = sheet_reference, ws = \"Sheet1\"))\n```\n\n```{r, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}\n(dados <- read_csv(\"2018-01-02-r4ds-slack-group_data.csv\"))\n```\n\nAs we can see, `gs_read()` loaded a `tibble` with `r nrow(dados)` rows and `r ncol(dados)` columns (variables). That's it! Now we have the data in RStudio to do as we like. \n\nWe can see that the columns' names are mostly questions, not even appearing in their whole extent when the tibble is printed. \n\n```{r, warning = FALSE}\n#original column names\ncolnames(dados)\n```\n\nThe reason for that is that this dataset is the result of a signup google form for the R4DS Slack Group. Google forms always include the question as the columns name for the resulting data.\n\nWe can assign shorter, meningful names to each column with `colnames()` so we don't have to type as much as we would have with those large column names. We'll also change **Cohort**'s numerical values into categories so it's explicit that the variable is categorical.\n\n```{r, warning = FALSE}\n#Assigning new names\ncolnames(dados) <- c(\"Type\", \"RStudio\", \"Stats\", \"Location\", \"Cohort\", \"lon\", \"lat\")\n\n#Converting variables into categorical\ndados$Cohort <- ifelse(dados$Cohort == 1, \"First\", \"Second\")\n```\n\n**RStudio** and **Stats** are also categorical, but have to remain as numbers, for that end I converted them to `factor`. Now we can have a deeper look at the data itself!\n\n# 3 Analyzing the data\n\nWell, now that we have our data ready we can start to satiate our curiosity! (I am curious, what about you?) \n\nAs we've already seen, our dataset consists of four variables:\n\n* **Type**: Each new member is asked to chose between joining as a Learner or a Mentor.\n\n* **RStudio**: How confortable each new member were working with RStudio at the moment of registration. The scale ranges from 1 (I know nothing) to 5 (I could teach this to others).\n\n* **Stats**: How confortable the person is with Statistics and/or Data Analysis. The scale is the same as the one in **RStudio**.\n\n* **Location**: A text response where each member is asked to type thei location of residence (City, State/Province, Country).\n\n* **Cohort**: This one is a variable indicating in which invitation each member joined the community.\n\n\nWhen I first thought about this post I intended to plot a map based on each member's **Location**, but then having a look at those other variables I considered also having a go with them. Let's have a look at these variables and see if we see anything interesting!\n\n## 3.1 Basic distributions\n\nFrom now on, if you're completely new to the `tidyverse` or to R itself, things might get shadier in terms of code, but I promise the visualizations will still be worth!\n\nThere are some variables that are not obligatory in the registration form for the R4DS Online Learning Community. That said, we shoul firstly have a look at `NA` values in our dataset.\n\n```{r, warning = FALSE}\n#Counting NA cases in each column\nmap_dbl(dados, ~sum(is.na(.) | . == \"\"))\n\n#Removing rows with only NA cases\ndados <- dados %>% filter(!is.na(Type))\n```\n\n\nWell, looks like there are 3 rows with no data at all and other 4 rows with no location. I removed rows with `NA` values in every column (no data at all). I didn't remove the rows with no location because they'll still contribute with other information and we can remove them right before using the **Location** column.\n\nNow that we've already cleaned the dataset from useless rows let's have a look at each variable separately and work from that! The plot tabs below show frequency distributions for each variable. It is worth to note that each tab in the tabset will display a different code, just as it displays a different graph.\n\n##{.tabset .tabset-fade .tabset-pills}\n\n### Cohorts\n```{r fig.align='center', warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>% \n  count(Cohort) %>%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %>%\n  ggplot(aes(Cohort, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by Cohort\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n### Types\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>% \n  count(Type) %>%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %>%\n  ggplot(aes(Type, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by Type\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n### RStudio\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>% \n  count(RStudio) %>%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %>%\n  ggplot(aes(RStudio, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by confortability with RStudio\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n### Stats\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>% \n  count(Stats) %>%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %>%\n  ggplot(aes(Stats, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by confortability with Statistics and Analysis\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n##\n\nIt is worth to note that these plots concern people who joined up, not necessarily active people. That said, the first plot shows us the frequency of each invitation (Cohort). We can also see at the **Type** frequency that there is a **`r round(sum(dados$Type == \"Learner\")/sum(dados$Type == \"Mentor\"), 1)`** Learner/Mentor ratio. \n\nAs for **RStudio**, it has mean **`r round(mean(dados$RStudio, na.rm = T), 1)`** with the frequency distribution rather stable despite not very higher frequencies around 3. Such behavior is stronger in **Stats** which has steeper increases as we approach the scale 3 from each side with a mean of **`r round(mean(as.numeric(dados$Stats), na.rm = T), 1)`**. We could say that our average member is a 3 in the comfortability both in terms of **RStudio** and **Stats**, however, in terms of **RStudio** there is more variability around 3 than **Stats**.\n\nBut before any thoughts about this info we should look deeper into the data, seeing if these frequencies change based on any variable's interaction.\n\n\n## 3.2 Vizualizing Relations\n\nWe've seen some basic frequency distribution, but are **RStudio**'s and **Stats**' frequency distributions equal when we plot them separately for each **Type**? \n\n## {.tabset .tabset-fade .tabset-pills}\n\n### RStudio by Type\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>%\n  count(RStudio, Type) %>%\n  group_by(Type) %>%\n  mutate(Count = n, Percentage = round(n/sum(n), 4)*100) %>%\n  group_by() %>%\n  ggplot(aes(RStudio, Percentage)) +\n  geom_bar(aes(labels = Count), fill = \"dodgerblue\", stat = \"identity\")+\n  facet_wrap(~Type) +\n  labs(title = \"RStudio scales frequency by Type\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n\n### Stats by Type\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>%\n  count(Stats, Type) %>%\n  group_by(Type) %>%\n  mutate(Count = n, Percentage = round(n/sum(n), 4)*100) %>%\n  group_by() %>%\n  ggplot(aes(Stats, Percentage)) +\n  geom_bar(aes(labels = Count), fill = \"dodgerblue\", stat = \"identity\")+\n  facet_wrap(~Type) +\n  labs(title = \"Stats scales frequency by Type\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n### RStudio vs Stats by Type\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>%\n  count(Type, RStudio, Stats) %>%\n  group_by(Type) %>%\n  mutate(Count = n, Percentage = round(n / sum(n), 4)*100) %>%\n  group_by() %>%\n  ggplot(aes(RStudio, Stats)) +\n  geom_tile(aes(fill = Percentage, labels = Count))+\n  facet_wrap(~Type) +\n  scale_fill_viridis_c()\n\ndiv(ggplotly(plot), align = \"center\")\n```\n\n##\n\nThe first two plots show us that there really are differences in frequency distributions when we look at each **Type** separately. _Mentors_ tend to choose higher scales of comfortability both in **RStudio** and **Stats**. \n\nHowever, it gets better when we look at the third graph, _Learners__ are quite frequent when the scales from 1 to 3. But there are a lot of configurations with people highly comfortable in working with **RStudio**, **Stats** or both! But now have a look at the _Mentors_' plot: there are no _Mentors_ who chose scales lower than 3 in BOTH **RStudio** and **Statistics**. And when there IS a 2, the other is a 4.\n\nWell, interestingly expected pattern, right? I mean, usually, someone who takes the position of _Mentor_ does it as a result of comfortability working with at least one: **RStudio** and **Stats**. As it is possible to see in the _Mentor_'s plot, most of them are pretty comfortable with both **RStudio** and **Stats**. \n\nI would guess that most _Mentors_ with lower scales chose this position for the challenge of improvement by helping others like I did!\n\nTo explore this data and perceive these patterns is cool, but the thing is: at the end of the day, everybody always help others to find a solution and to get better, no matter if it is a _Mentor_ or a _Learner_. **That why the R4DS Online Learning community is awesome!**\n\nWhat about NOW we see all these awesome members scattered on a map?\n\n## 3.2 Members' World Map\n\nThe best world maps are the interactive ones! We're going to need some coordinates so we can plot the map, right? But first, do you remember those **Location** rows with `NA` values? We're going to remove them now:\n\n```{r, warning = FALSE}\n#Remembering how many of them are there\nmap_dbl(dados, ~sum(is.na(.)))\n\n#Removing rows with NA in Location\nmp_dt <- filter(dados, !is.na(Location))\n```\n\nAnd there they go, they served their purpose and now are going to Valhalla. I assigned the cleaned data to the `mp_dt` variable, which stands for \"maps data\".\n\nNow, the `ggmap` package provides us with the `geocode()` function, which receives a vector of locations, try to find them in Google Map's database and returns the best guesses' coordinates. We're going to pass **Location** to the function:\n\n```{r eval=FALSE, warning = FALSE}\n#Passing locations to be searched in Google Maps\nlc_dt <- geocode(tolower(mp_dt$Location), source = \"google\")\n\nmp_dt <- cbind(mp_dt, lc_dt)\n```\n\nIt is worth to note that `ggmaps::geocode()` doesn't always return coordinates for all the locations. It happens for 2 reasons: (I) Since the locations are user-typed data, there might be incomplete data keeping `geocode()` from finding a fair enough answer; (II) At some times google will block some API calls returning \"OVER_QUERY_LIMIT\". \n\nAfter searching for a while about the (II) issue, I found out that it happens because of an intern query limit for the Google API's Calls. This issue might be solved for some locations by running the code and storing the non-error results in a separate variable and then repeating the process to try and obtain some other locations. Like the code below.\n\n```{r eval=FALSE, warning = FALSE}\n#Run these 2 lines once\nlongitude <- vector(\"double\", length(mp_dt$Location))\nlatitude <- vector(\"double\", length(mp_dt$Location))\n\n#Run these 3 lines as much as yout think is necessary for the result you want\nlc_dt <- geocode(tolower(mp_dt$Location), source = \"google\")\nlongitude <- ifelse(is.na(lc_dt$lon), longitude, lc_dt$lon)\nlatitude <- ifelse(is.na(lc_dt$lat), latitude, lc_dt$lat)\n\n#Check how many NAs are still there\nsum(is.na(longitude)) #Count\nmean(is.na(longitude)) #Percentage\n\n#If you're satisfied with the results, bind the coordinates with the data and go on\nmp_dt <- cbind(mp_dt, longitude, latitude)\n```\n\nI didn't wrap the code in a `while()` loop to prevent infinite loops in case the errors never get to 0%, as it could happen in the (I) case.\n\n```{r, echo=FALSE, message=FALSE, warning = FALSE}\n#write_csv(mp_dt, \"mp_dt.csv\")\nmp_dt <- dados\n```\n\nIn one of my recent Kernels as a newbie in Kaggle ( [Terrorism Worldwide - Exploratory Analysis](https://www.kaggle.com/neisse/terrorism-worldwide-exploratory-analysis) ) I wrote a not-that-good function to plot choropleth maps using the `leaflet` package. I'm going to use that function's code and modify it a little in a way that it plots our members with labelled markers instead of countries' polygons.\n\nIn order to plot a map using R, we need data concerning the map itself. The `maps::map()` function provides us with that.\n\n```{r, warning = FALSE}\n#Obtaining world map polygons\nworld <- maps::map(\"world\", fill = T, plot = F)\n\n#Creating a leaflet basic map\nm <- leaflet(world) %>% addTiles()\n```\n\nI used the `world` data to generate the basic layer with `leaflet()` and `addTiles()` from the `leaflet` package.\n\nAlso, since we're intended to plot markers for each member, we could use different colors for _Mentor_ and _Learners_ to see it on the map. \n\n```{r, warning = FALSE}\n#Creating categorical color palette\npal <- colorFactor( RColorBrewer::brewer.pal(2, \"Dark2\"), \n                   domain = mp_dt$Type, na.color = \"white\")\n```\n\n\nWe're going to generate some variable strings to be plotted in the map with the markers, these strings are generated with `sprintf()` and strings based on the C language.\n\n```{r, warning = FALSE}\n#Generating texts\nstrings <- sprintf(paste(\"<strong>%s</strong><br/><strong>%s</strong><br/>\", \n                         names(mp_dt[2]), \": %s<br/>\",\n                         names(mp_dt[3]), \": %s<br/>\",\n                         names(mp_dt[5]), \": %s\"),\n                   mp_dt[[4]], mp_dt[[1]], mp_dt[[2]], mp_dt[[3]], mp_dt[[5]])\n\n#Converting it to html format\nlabels <- strings %>% lapply(htmltools::HTML)\n```\n\nNow we only have to assemble all the parts in the base layer to have our map! I used `leaflet::addCircleMarkers()` to represent each member with a circle filled with colors based on **Type**. For guidance with the colors I added a legend with `leaflet::addLegend()`, and now our interactive map is complete!\n\n```{r warning=FALSE}\n\n#Adding polygon with the variable\nm %>% \n  addCircleMarkers(lng = mp_dt$lon, \n                      lat = mp_dt$lat, \n                      label = ~labels, radius = 1, \n                      color = pal(mp_dt$Type), opacity = 0.75) %>%\n  addLegend(\"bottomright\", pal = pal, values = ~mp_dt$Type,\n  title = names(mp_dt)[1],\n  opacity = 10)\n```\n\nEvery circle on the map is identified by all the varibales in the dataset! There are a lot of members on **North America** and **Europe**! **South America** and **India** come right after in the \"rank\".\n\n# 4 That's it!\n\nThis post is the result of an \"extended analysis for plotting a map out of curiosity\". I hope you liked it, specially the Slack members. If you have any feedback on your mind, share it in my twitter [\\@a_neisse](https://twitter.com/a_neisse) or on the Slack channel **neisse**!\n\n\n<style>\nbody {\ntext-align: justify}\n</style>","srcMarkdownNoYaml":"\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(fig.align = 'center')\n```\n\nBefore anything else, I tried HARD to make tabsets work with `blogdown` but it doesn't offer support to tabsets at all, nor folded code chunks, sadly. That said, this post might look longer than it really is! Unless you want to read every chunk of code, then it indeed IS as long as it seems.\n\n# 1 The group\n\nThe [R for Data Science](http://r4ds.had.co.nz/) book (R4DS) is a wonderfull book to introduce people both in the world of Data Science and R/RStudio. But something that improves the reading-through experience is the **R4DS Online Learning Community**. A Slack community that basically provides the means for people discuss about the book and anything that comes from that, like: sharing data sources, additional learning material, and helping each other to read it through. \n\nI joined it in the beginning, when Jesse Maegan [\\@kierisi](https://twitter.com/kierisi) invited people on Twitter. The story since then has been told in [her blog on Medium](https://medium.com/@kierisi). Now the community have had some improvements in structure (aaand some acquaintances in administration) so there can be weekly invitations. We're on the first round since these changes (the second since the beginning), if you have interest in the community don't be shy, join us! You can apply [HERE](https://docs.google.com/forms/d/e/1FAIpQLSeT3zfzjWxoaQ6RmUEdT9n0xtvkuSaMeBetDQLpzNJvGUB6IQ/viewform).\n\nAnd that's what this post is about, some visualizations on the signup data! We have some variables about our members that might render cool graphs and a members' world map! But first we'll have to download that data and we're going to do that from inside R! Let's dive in!\n\n# 2 The data and `googlesheets`\n\nThe group's admins maintain a google sheet with unidentified data on all the members with a few variables. This post will show some visualizations of this data. However, since it's maintained in Google Drive we'll first go through a quick tutorial on the `googlesheets` package to load worksheets directly from Google Drive to R(Studio).\n\nThat said, the packages we'll use in this post are the ones listed below:\n\n```{r, message=FALSE, warning=FALSE, results='hide'}\n#Used packages\nlibrary(tidyverse) #Data munging and vizualizing\nlibrary(googlesheets4) #Loading google sheets into R\nlibrary(plotly) #Plotting interactive graphs\nlibrary(shiny) #Package for R-based web apps, I use it for the dev() function to align plots produced with plotly\nlibrary(leaflet) #Ploting interactive maps\nlibrary(ggmap) #Package for google maps. Used here to obtain coordinates based on location\n```\n\nNow, `googlesheets` uses your Google Drive account to access the worksheets that you have access to and then work from that. So the first thing to do is share the data source into your Google Drive account. The googlesheet can be accessed here: [R4DS deidentified data](https://docs.google.com/spreadsheets/d/1L7lU5QqVfUtZXenFsGErPFS4n1jL4fixx9d3lrF94hU/edit#gid=0).\n\nWith the sheet in your account now it is possible to access it with `googlesheets`. The function `googlesheets::gs_ls()` will list all the sheets that you have access to. \n\nIt is worth to note that if it is the first time you are running `gs_ls()` you might have to authenticate your Google Drive Account and allow `googlesheets` to get data from your account.\n\n```{r eval=FALSE, warning = FALSE}\n#Listing the accessible google sheets\ngs_ls()\n```\n\nFrom the `gs_ls()` command you might have figured that the first column is the sheet's name (title). The title will be required for us to download the data. For the data the titl is \"R4DS deidentified data\n\nThe package `googlesheets` perform API calls to Google Drive to obtain the data from the specified sheet. Any API call depends on a set of parameters that, in this case, `googlesheets` will handle for us. The `gs_title()` function obtains the API parameters based on the worksheet's title, which we will provide.\n\n```{r, eval=FALSE, warning = FALSE}\n#Getting the API parameters for the desired google sheet\nsheet_reference <- gs_title(\"R4DS deidentified data\", verbose = F)\n```\n\nThe parameters were assigned to the `sheet_reference` variable. If everything went alright you will see a \"Sheet successfully identified: 'R4DS deidentified data'\" message in the prompt. Now, if you run `sheet_reference` you can see all the parameters.\n\n\nNow that you specified which google sheet you want, you may also list the sheets inside a worksheet with `gs_ws_ls()`. When the worksheet you are interested in has more than one sheet, this command will be useful for you to specify exactly which sheet you want to download. if not specified, by default the package will download the first sheet in the worksheet.\n\n```{r, eval=FALSE, warning = FALSE}\n#Listing all the sheets in our desired google sheet\ngs_ws_ls(sheet_reference)\n```\n\nFor the \"R4DS unidentified data\" google sheet there's just one sheet named \"Sheet1\". Since the package downloads the first sheet by default, there is no need for us to specify it.\n\nParameters at hand! Now we can download and have a first look at the data with `googlesheets::gs_read()` and assing it to the `dados` variable. The word \"dados\" is brazilian for data, thats my way of broadly naming the data while preventing conflicts with any function's \"data\" parameter. Consider it my nationality's contribution for this post! ;D\n\n```{r, eval=FALSE, message=FALSE, warning = FALSE}\n#Downloading the data from the google sheet\n(dados <- gs_read(ss = sheet_reference, ws = \"Sheet1\"))\n```\n\n```{r, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}\n(dados <- read_csv(\"2018-01-02-r4ds-slack-group_data.csv\"))\n```\n\nAs we can see, `gs_read()` loaded a `tibble` with `r nrow(dados)` rows and `r ncol(dados)` columns (variables). That's it! Now we have the data in RStudio to do as we like. \n\nWe can see that the columns' names are mostly questions, not even appearing in their whole extent when the tibble is printed. \n\n```{r, warning = FALSE}\n#original column names\ncolnames(dados)\n```\n\nThe reason for that is that this dataset is the result of a signup google form for the R4DS Slack Group. Google forms always include the question as the columns name for the resulting data.\n\nWe can assign shorter, meningful names to each column with `colnames()` so we don't have to type as much as we would have with those large column names. We'll also change **Cohort**'s numerical values into categories so it's explicit that the variable is categorical.\n\n```{r, warning = FALSE}\n#Assigning new names\ncolnames(dados) <- c(\"Type\", \"RStudio\", \"Stats\", \"Location\", \"Cohort\", \"lon\", \"lat\")\n\n#Converting variables into categorical\ndados$Cohort <- ifelse(dados$Cohort == 1, \"First\", \"Second\")\n```\n\n**RStudio** and **Stats** are also categorical, but have to remain as numbers, for that end I converted them to `factor`. Now we can have a deeper look at the data itself!\n\n# 3 Analyzing the data\n\nWell, now that we have our data ready we can start to satiate our curiosity! (I am curious, what about you?) \n\nAs we've already seen, our dataset consists of four variables:\n\n* **Type**: Each new member is asked to chose between joining as a Learner or a Mentor.\n\n* **RStudio**: How confortable each new member were working with RStudio at the moment of registration. The scale ranges from 1 (I know nothing) to 5 (I could teach this to others).\n\n* **Stats**: How confortable the person is with Statistics and/or Data Analysis. The scale is the same as the one in **RStudio**.\n\n* **Location**: A text response where each member is asked to type thei location of residence (City, State/Province, Country).\n\n* **Cohort**: This one is a variable indicating in which invitation each member joined the community.\n\n\nWhen I first thought about this post I intended to plot a map based on each member's **Location**, but then having a look at those other variables I considered also having a go with them. Let's have a look at these variables and see if we see anything interesting!\n\n## 3.1 Basic distributions\n\nFrom now on, if you're completely new to the `tidyverse` or to R itself, things might get shadier in terms of code, but I promise the visualizations will still be worth!\n\nThere are some variables that are not obligatory in the registration form for the R4DS Online Learning Community. That said, we shoul firstly have a look at `NA` values in our dataset.\n\n```{r, warning = FALSE}\n#Counting NA cases in each column\nmap_dbl(dados, ~sum(is.na(.) | . == \"\"))\n\n#Removing rows with only NA cases\ndados <- dados %>% filter(!is.na(Type))\n```\n\n\nWell, looks like there are 3 rows with no data at all and other 4 rows with no location. I removed rows with `NA` values in every column (no data at all). I didn't remove the rows with no location because they'll still contribute with other information and we can remove them right before using the **Location** column.\n\nNow that we've already cleaned the dataset from useless rows let's have a look at each variable separately and work from that! The plot tabs below show frequency distributions for each variable. It is worth to note that each tab in the tabset will display a different code, just as it displays a different graph.\n\n##{.tabset .tabset-fade .tabset-pills}\n\n### Cohorts\n```{r fig.align='center', warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>% \n  count(Cohort) %>%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %>%\n  ggplot(aes(Cohort, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by Cohort\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n### Types\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>% \n  count(Type) %>%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %>%\n  ggplot(aes(Type, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by Type\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n### RStudio\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>% \n  count(RStudio) %>%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %>%\n  ggplot(aes(RStudio, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by confortability with RStudio\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n### Stats\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>% \n  count(Stats) %>%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %>%\n  ggplot(aes(Stats, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by confortability with Statistics and Analysis\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n##\n\nIt is worth to note that these plots concern people who joined up, not necessarily active people. That said, the first plot shows us the frequency of each invitation (Cohort). We can also see at the **Type** frequency that there is a **`r round(sum(dados$Type == \"Learner\")/sum(dados$Type == \"Mentor\"), 1)`** Learner/Mentor ratio. \n\nAs for **RStudio**, it has mean **`r round(mean(dados$RStudio, na.rm = T), 1)`** with the frequency distribution rather stable despite not very higher frequencies around 3. Such behavior is stronger in **Stats** which has steeper increases as we approach the scale 3 from each side with a mean of **`r round(mean(as.numeric(dados$Stats), na.rm = T), 1)`**. We could say that our average member is a 3 in the comfortability both in terms of **RStudio** and **Stats**, however, in terms of **RStudio** there is more variability around 3 than **Stats**.\n\nBut before any thoughts about this info we should look deeper into the data, seeing if these frequencies change based on any variable's interaction.\n\n\n## 3.2 Vizualizing Relations\n\nWe've seen some basic frequency distribution, but are **RStudio**'s and **Stats**' frequency distributions equal when we plot them separately for each **Type**? \n\n## {.tabset .tabset-fade .tabset-pills}\n\n### RStudio by Type\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>%\n  count(RStudio, Type) %>%\n  group_by(Type) %>%\n  mutate(Count = n, Percentage = round(n/sum(n), 4)*100) %>%\n  group_by() %>%\n  ggplot(aes(RStudio, Percentage)) +\n  geom_bar(aes(labels = Count), fill = \"dodgerblue\", stat = \"identity\")+\n  facet_wrap(~Type) +\n  labs(title = \"RStudio scales frequency by Type\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n\n### Stats by Type\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>%\n  count(Stats, Type) %>%\n  group_by(Type) %>%\n  mutate(Count = n, Percentage = round(n/sum(n), 4)*100) %>%\n  group_by() %>%\n  ggplot(aes(Stats, Percentage)) +\n  geom_bar(aes(labels = Count), fill = \"dodgerblue\", stat = \"identity\")+\n  facet_wrap(~Type) +\n  labs(title = \"Stats scales frequency by Type\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n```\n\n### RStudio vs Stats by Type\n```{r, warning = FALSE}\n#Transforming the data and generating the plot\nplot <- dados %>%\n  count(Type, RStudio, Stats) %>%\n  group_by(Type) %>%\n  mutate(Count = n, Percentage = round(n / sum(n), 4)*100) %>%\n  group_by() %>%\n  ggplot(aes(RStudio, Stats)) +\n  geom_tile(aes(fill = Percentage, labels = Count))+\n  facet_wrap(~Type) +\n  scale_fill_viridis_c()\n\ndiv(ggplotly(plot), align = \"center\")\n```\n\n##\n\nThe first two plots show us that there really are differences in frequency distributions when we look at each **Type** separately. _Mentors_ tend to choose higher scales of comfortability both in **RStudio** and **Stats**. \n\nHowever, it gets better when we look at the third graph, _Learners__ are quite frequent when the scales from 1 to 3. But there are a lot of configurations with people highly comfortable in working with **RStudio**, **Stats** or both! But now have a look at the _Mentors_' plot: there are no _Mentors_ who chose scales lower than 3 in BOTH **RStudio** and **Statistics**. And when there IS a 2, the other is a 4.\n\nWell, interestingly expected pattern, right? I mean, usually, someone who takes the position of _Mentor_ does it as a result of comfortability working with at least one: **RStudio** and **Stats**. As it is possible to see in the _Mentor_'s plot, most of them are pretty comfortable with both **RStudio** and **Stats**. \n\nI would guess that most _Mentors_ with lower scales chose this position for the challenge of improvement by helping others like I did!\n\nTo explore this data and perceive these patterns is cool, but the thing is: at the end of the day, everybody always help others to find a solution and to get better, no matter if it is a _Mentor_ or a _Learner_. **That why the R4DS Online Learning community is awesome!**\n\nWhat about NOW we see all these awesome members scattered on a map?\n\n## 3.2 Members' World Map\n\nThe best world maps are the interactive ones! We're going to need some coordinates so we can plot the map, right? But first, do you remember those **Location** rows with `NA` values? We're going to remove them now:\n\n```{r, warning = FALSE}\n#Remembering how many of them are there\nmap_dbl(dados, ~sum(is.na(.)))\n\n#Removing rows with NA in Location\nmp_dt <- filter(dados, !is.na(Location))\n```\n\nAnd there they go, they served their purpose and now are going to Valhalla. I assigned the cleaned data to the `mp_dt` variable, which stands for \"maps data\".\n\nNow, the `ggmap` package provides us with the `geocode()` function, which receives a vector of locations, try to find them in Google Map's database and returns the best guesses' coordinates. We're going to pass **Location** to the function:\n\n```{r eval=FALSE, warning = FALSE}\n#Passing locations to be searched in Google Maps\nlc_dt <- geocode(tolower(mp_dt$Location), source = \"google\")\n\nmp_dt <- cbind(mp_dt, lc_dt)\n```\n\nIt is worth to note that `ggmaps::geocode()` doesn't always return coordinates for all the locations. It happens for 2 reasons: (I) Since the locations are user-typed data, there might be incomplete data keeping `geocode()` from finding a fair enough answer; (II) At some times google will block some API calls returning \"OVER_QUERY_LIMIT\". \n\nAfter searching for a while about the (II) issue, I found out that it happens because of an intern query limit for the Google API's Calls. This issue might be solved for some locations by running the code and storing the non-error results in a separate variable and then repeating the process to try and obtain some other locations. Like the code below.\n\n```{r eval=FALSE, warning = FALSE}\n#Run these 2 lines once\nlongitude <- vector(\"double\", length(mp_dt$Location))\nlatitude <- vector(\"double\", length(mp_dt$Location))\n\n#Run these 3 lines as much as yout think is necessary for the result you want\nlc_dt <- geocode(tolower(mp_dt$Location), source = \"google\")\nlongitude <- ifelse(is.na(lc_dt$lon), longitude, lc_dt$lon)\nlatitude <- ifelse(is.na(lc_dt$lat), latitude, lc_dt$lat)\n\n#Check how many NAs are still there\nsum(is.na(longitude)) #Count\nmean(is.na(longitude)) #Percentage\n\n#If you're satisfied with the results, bind the coordinates with the data and go on\nmp_dt <- cbind(mp_dt, longitude, latitude)\n```\n\nI didn't wrap the code in a `while()` loop to prevent infinite loops in case the errors never get to 0%, as it could happen in the (I) case.\n\n```{r, echo=FALSE, message=FALSE, warning = FALSE}\n#write_csv(mp_dt, \"mp_dt.csv\")\nmp_dt <- dados\n```\n\nIn one of my recent Kernels as a newbie in Kaggle ( [Terrorism Worldwide - Exploratory Analysis](https://www.kaggle.com/neisse/terrorism-worldwide-exploratory-analysis) ) I wrote a not-that-good function to plot choropleth maps using the `leaflet` package. I'm going to use that function's code and modify it a little in a way that it plots our members with labelled markers instead of countries' polygons.\n\nIn order to plot a map using R, we need data concerning the map itself. The `maps::map()` function provides us with that.\n\n```{r, warning = FALSE}\n#Obtaining world map polygons\nworld <- maps::map(\"world\", fill = T, plot = F)\n\n#Creating a leaflet basic map\nm <- leaflet(world) %>% addTiles()\n```\n\nI used the `world` data to generate the basic layer with `leaflet()` and `addTiles()` from the `leaflet` package.\n\nAlso, since we're intended to plot markers for each member, we could use different colors for _Mentor_ and _Learners_ to see it on the map. \n\n```{r, warning = FALSE}\n#Creating categorical color palette\npal <- colorFactor( RColorBrewer::brewer.pal(2, \"Dark2\"), \n                   domain = mp_dt$Type, na.color = \"white\")\n```\n\n\nWe're going to generate some variable strings to be plotted in the map with the markers, these strings are generated with `sprintf()` and strings based on the C language.\n\n```{r, warning = FALSE}\n#Generating texts\nstrings <- sprintf(paste(\"<strong>%s</strong><br/><strong>%s</strong><br/>\", \n                         names(mp_dt[2]), \": %s<br/>\",\n                         names(mp_dt[3]), \": %s<br/>\",\n                         names(mp_dt[5]), \": %s\"),\n                   mp_dt[[4]], mp_dt[[1]], mp_dt[[2]], mp_dt[[3]], mp_dt[[5]])\n\n#Converting it to html format\nlabels <- strings %>% lapply(htmltools::HTML)\n```\n\nNow we only have to assemble all the parts in the base layer to have our map! I used `leaflet::addCircleMarkers()` to represent each member with a circle filled with colors based on **Type**. For guidance with the colors I added a legend with `leaflet::addLegend()`, and now our interactive map is complete!\n\n```{r warning=FALSE}\n\n#Adding polygon with the variable\nm %>% \n  addCircleMarkers(lng = mp_dt$lon, \n                      lat = mp_dt$lat, \n                      label = ~labels, radius = 1, \n                      color = pal(mp_dt$Type), opacity = 0.75) %>%\n  addLegend(\"bottomright\", pal = pal, values = ~mp_dt$Type,\n  title = names(mp_dt)[1],\n  opacity = 10)\n```\n\nEvery circle on the map is identified by all the varibales in the dataset! There are a lot of members on **North America** and **Europe**! **South America** and **India** come right after in the \"rank\".\n\n# 4 That's it!\n\nThis post is the result of an \"extended analysis for plotting a map out of curiosity\". I hope you liked it, specially the Slack members. If you have any feedback on your mind, share it in my twitter [\\@a_neisse](https://twitter.com/a_neisse) or on the Slack channel **neisse**!\n\n\n<style>\nbody {\ntext-align: justify}\n</style>"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2017-12-15-r4ds-slack-group.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":{"light":"cosmo","dark":["cosmo","../../theme-dark.scss"]},"title":"**R4DS Slack Community data**","description":"Data analysis of signup data from R4DS","author":"Anderson Neisse","date":"2017-12-15","categories":["Statistics","Data Science","R4DS","Maps","Exploratory Analysis"],"image":"featured.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}