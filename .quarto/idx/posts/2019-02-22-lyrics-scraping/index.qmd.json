{"title":"**Scraping lyrics from Vagalume**","markdown":{"yaml":{"title":"**Scraping lyrics from Vagalume**","description":"Using R to webscrape vagalume.com","author":"Anderson Neisse","date":"2019-10-02","categories":["Webscraping","Music","Code"],"image":"featured.png","title-block-banner":"featured.png","include-in-header":[{"text":"<style>\n.quarto-title-block .quarto-title-banner {\n  background-position-y: center;\n  height: 200px;\n}\n</style>\n"}],"toc":true,"execute":{"eval":false}},"headingText":"1 Scraping artists data","containsRefs":false,"markdown":"\n\n```{r, echo = FALSE, message = FALSE, warning = FALSE}\nknitr::opts_chunk$set(fig.align = 'center')\n```\n\n  Today I woke up with a desire to stretch my web scraping skills and willing to do so while listening to some music, so why not scrap some music data? In this post I will scrap som data on artists and their lyrics so in a future post I plan on having some visualizations on the data as well as train a LSTM on the lyrics and maybe make it compose some new ones!\n\n  This whole post is about scraping the data from a Brazilian lyrics website called [Vagalume](https://www.vagalume.com.br) (literally Firefly in PT-BR), which stores music lyrics for a lot of artists, not only Brazilians. The site's evaluations of popularity might result on some cool visualizations. And of course, it's accessed almost exclusively by brazilians, so it will be informative about their tastes, on some level.\n  \n  NOTE: The datasets generated by this posts are stored in Kaggle [here](http://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres), in case you want them to do some analysis.\n\n  Some packages we will need for this task:\n\n```{r, message=FALSE,warning=FALSE}\nlibrary(tidyverse)\nlibrary(httr)\nlibrary(xml2)\nlibrary(rvest)\nlibrary(furrr)\n```\n\n  The packages `httr`, `xml2` and `rvest` are the main ones for html-based web scraping these days. Loading `tidyverse` for all reasons and `furrr` for combination of `future` and `purrr` packages.\n\n  The website has a A-to-Z list of artists, with a link for each letter, then a link for each artist with such starting letter, which contains a list of music names with lyrics \"inside\". It also displays the artists in music genre sections, which I will use, they work very similarly to the A-to-Z sections. I'll need to go through lots of links in order to collect all the data I want. \n\n\n  First let's grab data on the artists, it's a first step to know how many artists and musics the site has. At the end of this section we should have a data frame containing data on the artists on the website.\n  \n  To obtain our data frame we will need a list of links to artists' pages on the site as well as a scrapper function to return the data we want from them. The scrapper is the function that will receive a link a an artist's page and return the data we want. Using the `rvest` package we can build a scrapper. \n  \n  Let's first build a scrapper that returns the data we need from a single artist. For that purpose I'm going to use the page for [Green Day](https://www.vagalume.com.br/green-day/).\n\n  We can see that it contains the bands name, the list of all the lyrics, and a subpage \"Popularidade\" with the popularity history of the band according to the site's access history.  Those pages are constant across all the artists while the others aren't as some artists might not have them. Now, there are some information that we can get for individual artists:\n  \n  - Artist's name;\n  - Genre labels;\n  - Number of musics in the site;\n  - Popularity;\n\n  Let's get scraping!\n  \n## Building the scrapper\n\n  I'm going to start with scraping the band's name to illustrate how `rvest` works with web scraping. The code below does that job for us. The `read_html()` parses a html page into R. Then, `html_nodes()` extracts a specific part (node) from the page that contains the data we want. You can define the node's name with [Selector Gadget](https://selectorgadget.com/) or by inspecting the page's code in the browser. Also, a fun and very instructive tutorial on selecting css labels is [CSS Dinner](http://flukeout.github.io/#). Finally, we turn the lyric into text, removing line breaks and this sort of stuff with `html_text`.\n  \n```{r}\n# Getting an artists name\nread_html(\"https://www.vagalume.com.br/green-day/\") %>%\n    html_nodes(\".darkBG\") %>%\n    html_text()\n```\n\n  There, now we will try to get the labels for music genre for the band:\n  \n```{r}\n# Scraping data on genre\nread_html(\"https://www.vagalume.com.br/green-day/\") %>%\n    html_nodes(\".subHeaderTags\") %>%\n    as_list() %>%\n    unlist() %>%\n    paste(collapse = \"; \")\n```\n  \n  In this case we need to parse it to a list using `xml2::as_list()` and then use `apply::unlist()` in order to transform the html to a vector. If we used `rvest::html_text()` it would have concatenated all the labels with `sep = \"\"`. Green day has the labels: \"Rock\"; \"Punk Rock\" and \"Rock Alternativo\" (Alternative Rock). \n  \n  Now let's try and get number of musics for the band. The main page for them has the list of all their songs, we can get them and them store their count. \n  \n```{r}\nread_html(\"https://www.vagalume.com.br/green-day/\") %>%\n    html_nodes(\".nameMusic\") %>%\n    html_text() %>%\n    unique() %>%\n    length()\n```\n\n  The first line of code above, if ran alone, returns a vector with all the music names in the page. However before the list of all songs there is a top 25 of the artist's songs, so we remove duplicates by using `unique()` and then count how many songs are there using `length()` applied to the resulting vector of unique song names.\n  \n  Now, last but not least important, the artist's popularity. This one is stored in a sub-page (\\\\popularidade\\\\) of each artist. In the end of the page it shows the current popularity in a pop which always begins with the word \"Atualmente\".\n  \n```{r}\nread_html(\"https://www.vagalume.com.br/green-day/popularidade/\") %>%\n    html_nodes(\".text\") %>%\n    html_text()\n```\n  \n  So we need to filter the one that begins with that word (which means 'Currently' by the way). And then we need to extract the value of popularity (9.0 in this case) which is always in between the words \"em\" and \"pontos\".\n  \n```{r}\nread_html(\"https://www.vagalume.com.br/green-day/popularidade/\") %>%\n    html_nodes(\".text\") %>%\n    html_text() %>%\n    # Extracting last phrase\n    tail(1) %>%\n    # Using Regular Expressions to remove the number\n    str_extract(., \"(?<=está em )(.*)(?= pontos)\") %>%\n    # Replacing brazilian decimal \",\" by \".\" and arsing to numeric\n    str_replace(\",\", \".\") %>%\n    as.numeric()\n```\n\n  It took some work but we got the popularity for the band, shame it's rounded up to an integer, but it won't be completely useless. \n  \n  Now we know how to get every information we need from a single artist, so let's build our scraper function:\n  \n```{r}\nscrap_artist <- function(artist_link) {\n    # Reading the entire pages\n    page <- read_html(paste0(\"https://www.vagalume.com.br\", artist_link))\n    pop_page <- read_html(paste0(\"https://www.vagalume.com.br\", artist_link, \"popularidade/\"))\n\n    # Getting desired variables\n    A <- page %>%\n        html_nodes(\".darkBG\") %>%\n        html_text()\n\n    G <- page %>%\n        html_nodes(\".subHeaderTags\") %>%\n        as_list() %>%\n        unlist() %>%\n        paste(collapse = \"; \")\n\n    S <- page %>%\n        html_nodes(\".nameMusic\") %>%\n        html_text() %>%\n        unique() %>%\n        length()\n\n    P <- pop_page %>%\n        html_nodes(\".text\") %>%\n        html_text() %>%\n        tail(1) %>%\n        str_extract(., \"(?<=está em )(.*)(?= pontos)\") %>%\n        str_replace(\",\", \".\") %>%\n        as.numeric()\n\n    # Creating tibble\n    res <- tibble(\n        Artist = A,\n        Genres = G,\n        Songs = S,\n        Popularity = P,\n        Link = artist_link\n    )\n    return(res)\n}\n\n# Testing the scrapper function\nscrap_artist(\"/green-day/\")\n```\n\n  Nice, a single function that receives a link to an artist and returns all the data we defined for that artist. Notice that I defined a function such as it receives only the important part of the link, which is what we will scrap from the site in order to form our data frame.\n\n## Obtaining links of artists\n  \n  Now, since this post is just to stretch my web scraping skills and have some fun I won't scrap data from all the artists, which could take ages as the process depends not only on the processing power but also on the internet connection and time to get to each link. However, we will get some music genres to play:\n  \n  - Rock\n  - Hip Hop\n  - Pop music\n  - Sertanejo (Basically the Brazilian version of Country Music)\n  - Funk Carioca (Originated 60s US Funk, a completely different genre in Brazil nowadays)\n  - Samba (Typical Brazilian music)\n  \n  In order to map our scrapper to artists we need to have the link to their pages in the website. For that we need to get the links for music genre sections we want to scrap:\n\n```{r}\npage <- read_html(\"https://www.vagalume.com.br/browse/style/\")\n\nstyles <- page %>%\n    html_nodes(\".itemTitle\") %>%\n    html_text() %>%\n    str_replace(., \"/\", \"-\") %>%\n    str_replace(., \"á\", \"a\") %>%\n    str_replace(., \"é\", \"e\") %>%\n    str_replace(., \"â\", \"a\") %>%\n    str_replace(., \"ó\", \"o\") %>%\n    str_replace_all(., \"ú\", \"u\") %>%\n    str_replace(., \"&\", \"-n-\") %>%\n    str_replace(., \" \", \"-\") %>%\n    str_replace(., \"J-Pop-J-Rock\", \"j-pop\") %>%\n    str_replace(., \"Gospel-Religioso\", \"gospel\") %>%\n    str_replace(., \"Romantico\", \"romantica\") %>%\n    tolower()\n\nsection_links <- paste0(\"/browse/style/\", styles, \".html\")\n```\n\n  Now, in one specific page (\"Rock\"), we need to figure out how to get the list of artists. We need to do it since we will need a list of all the artists to scrap the data with the `scrap_artist` function. Let's start by getting the objects with `class=moreNamesContainer` (\".moreNamesContainer\") as identified in the page's code with \\textbf{Selector Gadget}.\n  \n  The table containing all the names for each section page is converted to a list (which unfortunately has 4 levels), so the code below is able to extract the links. It's already inside a function `artist_links` that will be useful for us.\n  \n```{r}\nget_artist_links <- function(section_link) {\n    # Reading the page\n    page <- read_html(paste0(\"https://www.vagalume.com.br\", section_link))\n\n    # Removing desired node as a list\n    nameList <- page %>%\n        html_nodes(\".moreNamesContainer\") %>%\n        as_list()\n\n    # Removing undesired list levels and extracting 'href' attribute\n    # 'NOT ELEGANT AT ALL' ALLERT\n    artist_links <- nameList %>%\n        unlist(recursive = F) %>% # Removing first undesired level\n        unlist(recursive = F) %>% # Removing second undesired level\n        unlist(recursive = F) %>% # Removig third undesired level, ugh\n        map(., ~ attr(., \"href\")) %>% # Mapping through list to extract attrs\n        unlist() %>%\n        as.character() # Removing names and parsing to a vector\n\n    return(unique(artist_links))\n}\n\nget_artist_links(section_link = \"/browse/style/rock.html\") %>% head(10)\n```\n\n  Ouch, what inelegant code, probably due to my lack of knowledge on `purrr`, I definitely need to get deeper than `map_*()` on this package soon. But ok, let's map it through all values of `section_links` in order to obtain the links to all artists in the website. \n  \n  In order to increase performance we will use `furrr::future_map()` which is a function that conveniently combines `purrr::map()` with the parallel processing provided by the `future` package.\n\n```{r}\n# Planning parallel processing\nplan(multisession)\n\n# Getting all artists' links from the website\n#all_artists_links <- get_artist_links(\"/browse/style/sertanejo.html\")\nall_artists_links <- future_map(section_links, ~get_artist_links(.)) %>% \nunlist()\n```\n  \n  I got $3.255$ links when I scrapped the data in February 10th in 2019. That leads us to the first big step in this endeavour, our Artists dataset.\n  \n## Building the dataset\n\n  Now that we have the scrapper function `scrap_artist` and a list to map it through, we can build our dataset. Since our scrapper returns a tibble i will use the `map_dfr` variation which already binds the resulting data frames by rows. Of course there is a `furrr::future_map_dfr()` to save is since this operation is going to be much more demanding than the last one.\n  \n```{r, eval=FALSE}\n# De\np_scrap_artist <- possibly(scrap_artist, \n                           otherwise = tibble(Artist = NA, \n                                              Genres = NA, \n                                              Songs = NA, \n                                              Popularity = NA, \n                                              Link = NA))\n\n# Planning parallel processing\nplan(multisession)\n\n# Getting all artists' links from the website\nall_artists <- future_map_dfr(all_artists_links, ~p_scrap_artist(.))\n```\n\n\n  It took around 16 minutes to finish this data scraping on my gaming laptop, it may depend on internet connection and processing power. Notice that I used `possibly()` to create a new function. It wraps a function and in case it returns an error it won't stop our code, instead it will return what I passed to the `otherwise` argument, which is a tibble of `NA`s. It's possible to use the argument `.progress = TRUE` in the `future_map_*` functions in order to check the map procedure progress in case of big operations like this one.\n  \n  Originally i wanted to maintain only some genres as shown bellow, but on the most recent revision of the data on Kaggle i scraped all the genres, so the code below is commented but you can use it if you want specific genres to be scraped:\n\n```{r, warning=FALSE, eval=FALSE}\n# Selecting genre labels to keep\n#genres_keep <- c(\"Rock\", \"Hip Hop\", \"Pop\", \"Sertanejo\", \"Funk Carioca\", \"Samba\")\n\n\n# Removing other genre labels\n#all_artists_fixed <- all_artists %>% \n#  separate(Genres, c(\"G1\", \"G2\", \"G3\"), sep = \"; \") %>%  # Separating Genres variable\n#  gather(G1:G3, key = \"G\", value = \"Genre\") %>% select(-G) %>% \n#  filter(Genre %in% genres_keep)\n```\n  \n  There, now we have some data on all artists on the Genres I specified. The number of rows grew up to $3622$ because of artists that had more than one label, I will rather maintain it this way since removing the duplicates might bias the results to one genre, at least this way the artists weight equally both genres they are present in. The dataset on artists is built! We can do a last step on the data scraping before we stop for today: Scraping their lyrics!\n\n# 2 Scraping lyrics data\n\n  I decided to already scrap some data on the lyrics to use in the future analysis post, some text analysis, maybe some sentiment analysis and then I have plans for a lyrics-composing AI, I will create a little monster that will ruin the music industry, ok I'll stop dreaming. Let's do it!\n  \n## Building the scrapper\n\n  We are going to do it similarly to how we did it with the artists: do a scrapper to get all the individual links then map throuhg it. I already built a lyrics scraper `get_lyric()`, that returns the lyric based on the songs' link. the code below defines it:\n\n```{r}\n# Extracts a single lyric from a song link\nget_lyric <- function(song_link){\n  \n  # Reading the html page\n  lyric <- read_html(paste0(\"https://www.vagalume.com.br\", song_link)) %>% \n    html_nodes(\"#lyrics\") %>% \n    html_text2()\n  \n  return(lyric)\n}\n\n# Testing it on Holiday from Green Day\nget_lyric(song_link = \"/green-day/holiday.html\")\n```\n  \n  From this specific lyric we can see that there are still some special characters. Instead of just treating this \"\\\\\" issue, I'll remove all special characters when analyzing the lyrics. Better, right? Let's get to the next step on our scraping adventure.\n  \n  \n## Links to scrap the data from\n  \n  Now we need the list of songs to map the scraper through. The code below defines `get_lyrics_links()`, which receives the link to the artist then returns a tibble with name and link to the music. It also returns the link to the artist that was passed, in order to use it as an ID for the artist dataset in the future. \n\n```{r}\n# Extracts all lyrics from an artist link - uses get_lyric()\nget_lyrics_links <- function(artist_link){\n  \n  # Reading the html page on the artist\n  page <- read_html(paste0(\"https://www.vagalume.com.br\", artist_link))\n  \n  # Obtaining all the musics' links -\n  music_name_node <- page %>% html_nodes(\".nameMusic\")\n  music_names <- music_name_node %>% html_text()\n  music_links <- music_name_node %>% html_attr(\"href\")\n  \n  # Building final tibble\n  res <- tibble(ALink = rep(artist_link, length(music_names)),\n                SName = music_names,\n                SLink = music_links)\n  \n  return(res)\n}\n\n# Testing final function\nget_lyrics_links(\"/green-day/\") %>% head(5)\n```\n\n  Notice that I use `html_attr(\"href\")` to extract the attribute \"href\" from the html object, which is the link to the song page. Let's map this function through all the artists we have in order to obtain the links to the musics:\n  \n```{r, eval=FALSE}\n# Generating safe version of get_lyrics_links()\np_get_lyrics_links <- possibly(get_lyrics_links, \n                               otherwise = tibble(ALink = NA,\n                                                  SName = NA))\n\n# Returning all the links to the musics\nplan(multisession)\nall_songs_links <- future_map_dfr(all_artists_links, ~p_get_lyrics_links(.))\n```\n  \n  After a 82.98 seconds wait on my case we found out that there are $209.522$ songs for our $3.355$ artists. \n\n## Building the lyrics dataset  \n  \n  Now all we gotta do is map hour scraper `get_lyric()` through all this songs to get our final lyrics dataset.\n  \n```{r, eval=FALSE}\n# Gerenating a safe version with possibly()\np_get_lyric <- possibly(get_lyric, otherwise = NA)\n\n# Mapping it through the all_artists_links vector\nplan(multisession)\nall_lyrics <- future_map_chr(all_songs_links$SLink, ~p_get_lyric(.)[1], .progress = TRUE)\n\n# Adding it to the dataframe\nall_songs_links$Lyric <- all_lyrics %>% str_replace_all(\"\\\\. \\\\. \", \". \")\n```\n\n  This one took around 9,064.08 seconds (151 minutes) to finish, quite some time! \n  \n  You might be asking why not build a single function to scrap the lyrics. Well, I tried it at first, a function that took the artist link, mapped through all song link inside it and returned a tibble with all the artist lyrics. But it was taking hours to run iterally. I think it happens because `furrr` can't optimize well functions that move great pieces of data around. Also, it was a function with a map call inside, seems like it's not possible to call a future within a future call so I had to separate them in order to optimize running time.\n  \n  Finally, we can use the `cld2` package to infer each lyric's language to make future analysis easier:\n  \n```{r, eval=FALSE}\nlibrary(cld2)\n\nall_songs_links <- all_songs_links %>% \n  mutate(language = detect_language(Lyric))\n```\n\n# 3 That's it!\n\n  Both datasets can be foun on my Kaggle datasets page, here. Feel free to analyze it the way you want!\n\n  This post is the first of two parts of the music analysis. In the next one i plan on briefly analyzing this data and training a LSTM in the lyrics for each genre. I hope you liked it so far! If you have any feedback on your mind, share it in my twitter [\\@a_neisse](https://twitter.com/a_neisse) or on the contact section of this website.\n\n<style>\nbody {\ntext-align: justify}\n</style>","srcMarkdownNoYaml":"\n\n```{r, echo = FALSE, message = FALSE, warning = FALSE}\nknitr::opts_chunk$set(fig.align = 'center')\n```\n\n  Today I woke up with a desire to stretch my web scraping skills and willing to do so while listening to some music, so why not scrap some music data? In this post I will scrap som data on artists and their lyrics so in a future post I plan on having some visualizations on the data as well as train a LSTM on the lyrics and maybe make it compose some new ones!\n\n  This whole post is about scraping the data from a Brazilian lyrics website called [Vagalume](https://www.vagalume.com.br) (literally Firefly in PT-BR), which stores music lyrics for a lot of artists, not only Brazilians. The site's evaluations of popularity might result on some cool visualizations. And of course, it's accessed almost exclusively by brazilians, so it will be informative about their tastes, on some level.\n  \n  NOTE: The datasets generated by this posts are stored in Kaggle [here](http://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres), in case you want them to do some analysis.\n\n  Some packages we will need for this task:\n\n```{r, message=FALSE,warning=FALSE}\nlibrary(tidyverse)\nlibrary(httr)\nlibrary(xml2)\nlibrary(rvest)\nlibrary(furrr)\n```\n\n  The packages `httr`, `xml2` and `rvest` are the main ones for html-based web scraping these days. Loading `tidyverse` for all reasons and `furrr` for combination of `future` and `purrr` packages.\n\n  The website has a A-to-Z list of artists, with a link for each letter, then a link for each artist with such starting letter, which contains a list of music names with lyrics \"inside\". It also displays the artists in music genre sections, which I will use, they work very similarly to the A-to-Z sections. I'll need to go through lots of links in order to collect all the data I want. \n\n# 1 Scraping artists data\n\n  First let's grab data on the artists, it's a first step to know how many artists and musics the site has. At the end of this section we should have a data frame containing data on the artists on the website.\n  \n  To obtain our data frame we will need a list of links to artists' pages on the site as well as a scrapper function to return the data we want from them. The scrapper is the function that will receive a link a an artist's page and return the data we want. Using the `rvest` package we can build a scrapper. \n  \n  Let's first build a scrapper that returns the data we need from a single artist. For that purpose I'm going to use the page for [Green Day](https://www.vagalume.com.br/green-day/).\n\n  We can see that it contains the bands name, the list of all the lyrics, and a subpage \"Popularidade\" with the popularity history of the band according to the site's access history.  Those pages are constant across all the artists while the others aren't as some artists might not have them. Now, there are some information that we can get for individual artists:\n  \n  - Artist's name;\n  - Genre labels;\n  - Number of musics in the site;\n  - Popularity;\n\n  Let's get scraping!\n  \n## Building the scrapper\n\n  I'm going to start with scraping the band's name to illustrate how `rvest` works with web scraping. The code below does that job for us. The `read_html()` parses a html page into R. Then, `html_nodes()` extracts a specific part (node) from the page that contains the data we want. You can define the node's name with [Selector Gadget](https://selectorgadget.com/) or by inspecting the page's code in the browser. Also, a fun and very instructive tutorial on selecting css labels is [CSS Dinner](http://flukeout.github.io/#). Finally, we turn the lyric into text, removing line breaks and this sort of stuff with `html_text`.\n  \n```{r}\n# Getting an artists name\nread_html(\"https://www.vagalume.com.br/green-day/\") %>%\n    html_nodes(\".darkBG\") %>%\n    html_text()\n```\n\n  There, now we will try to get the labels for music genre for the band:\n  \n```{r}\n# Scraping data on genre\nread_html(\"https://www.vagalume.com.br/green-day/\") %>%\n    html_nodes(\".subHeaderTags\") %>%\n    as_list() %>%\n    unlist() %>%\n    paste(collapse = \"; \")\n```\n  \n  In this case we need to parse it to a list using `xml2::as_list()` and then use `apply::unlist()` in order to transform the html to a vector. If we used `rvest::html_text()` it would have concatenated all the labels with `sep = \"\"`. Green day has the labels: \"Rock\"; \"Punk Rock\" and \"Rock Alternativo\" (Alternative Rock). \n  \n  Now let's try and get number of musics for the band. The main page for them has the list of all their songs, we can get them and them store their count. \n  \n```{r}\nread_html(\"https://www.vagalume.com.br/green-day/\") %>%\n    html_nodes(\".nameMusic\") %>%\n    html_text() %>%\n    unique() %>%\n    length()\n```\n\n  The first line of code above, if ran alone, returns a vector with all the music names in the page. However before the list of all songs there is a top 25 of the artist's songs, so we remove duplicates by using `unique()` and then count how many songs are there using `length()` applied to the resulting vector of unique song names.\n  \n  Now, last but not least important, the artist's popularity. This one is stored in a sub-page (\\\\popularidade\\\\) of each artist. In the end of the page it shows the current popularity in a pop which always begins with the word \"Atualmente\".\n  \n```{r}\nread_html(\"https://www.vagalume.com.br/green-day/popularidade/\") %>%\n    html_nodes(\".text\") %>%\n    html_text()\n```\n  \n  So we need to filter the one that begins with that word (which means 'Currently' by the way). And then we need to extract the value of popularity (9.0 in this case) which is always in between the words \"em\" and \"pontos\".\n  \n```{r}\nread_html(\"https://www.vagalume.com.br/green-day/popularidade/\") %>%\n    html_nodes(\".text\") %>%\n    html_text() %>%\n    # Extracting last phrase\n    tail(1) %>%\n    # Using Regular Expressions to remove the number\n    str_extract(., \"(?<=está em )(.*)(?= pontos)\") %>%\n    # Replacing brazilian decimal \",\" by \".\" and arsing to numeric\n    str_replace(\",\", \".\") %>%\n    as.numeric()\n```\n\n  It took some work but we got the popularity for the band, shame it's rounded up to an integer, but it won't be completely useless. \n  \n  Now we know how to get every information we need from a single artist, so let's build our scraper function:\n  \n```{r}\nscrap_artist <- function(artist_link) {\n    # Reading the entire pages\n    page <- read_html(paste0(\"https://www.vagalume.com.br\", artist_link))\n    pop_page <- read_html(paste0(\"https://www.vagalume.com.br\", artist_link, \"popularidade/\"))\n\n    # Getting desired variables\n    A <- page %>%\n        html_nodes(\".darkBG\") %>%\n        html_text()\n\n    G <- page %>%\n        html_nodes(\".subHeaderTags\") %>%\n        as_list() %>%\n        unlist() %>%\n        paste(collapse = \"; \")\n\n    S <- page %>%\n        html_nodes(\".nameMusic\") %>%\n        html_text() %>%\n        unique() %>%\n        length()\n\n    P <- pop_page %>%\n        html_nodes(\".text\") %>%\n        html_text() %>%\n        tail(1) %>%\n        str_extract(., \"(?<=está em )(.*)(?= pontos)\") %>%\n        str_replace(\",\", \".\") %>%\n        as.numeric()\n\n    # Creating tibble\n    res <- tibble(\n        Artist = A,\n        Genres = G,\n        Songs = S,\n        Popularity = P,\n        Link = artist_link\n    )\n    return(res)\n}\n\n# Testing the scrapper function\nscrap_artist(\"/green-day/\")\n```\n\n  Nice, a single function that receives a link to an artist and returns all the data we defined for that artist. Notice that I defined a function such as it receives only the important part of the link, which is what we will scrap from the site in order to form our data frame.\n\n## Obtaining links of artists\n  \n  Now, since this post is just to stretch my web scraping skills and have some fun I won't scrap data from all the artists, which could take ages as the process depends not only on the processing power but also on the internet connection and time to get to each link. However, we will get some music genres to play:\n  \n  - Rock\n  - Hip Hop\n  - Pop music\n  - Sertanejo (Basically the Brazilian version of Country Music)\n  - Funk Carioca (Originated 60s US Funk, a completely different genre in Brazil nowadays)\n  - Samba (Typical Brazilian music)\n  \n  In order to map our scrapper to artists we need to have the link to their pages in the website. For that we need to get the links for music genre sections we want to scrap:\n\n```{r}\npage <- read_html(\"https://www.vagalume.com.br/browse/style/\")\n\nstyles <- page %>%\n    html_nodes(\".itemTitle\") %>%\n    html_text() %>%\n    str_replace(., \"/\", \"-\") %>%\n    str_replace(., \"á\", \"a\") %>%\n    str_replace(., \"é\", \"e\") %>%\n    str_replace(., \"â\", \"a\") %>%\n    str_replace(., \"ó\", \"o\") %>%\n    str_replace_all(., \"ú\", \"u\") %>%\n    str_replace(., \"&\", \"-n-\") %>%\n    str_replace(., \" \", \"-\") %>%\n    str_replace(., \"J-Pop-J-Rock\", \"j-pop\") %>%\n    str_replace(., \"Gospel-Religioso\", \"gospel\") %>%\n    str_replace(., \"Romantico\", \"romantica\") %>%\n    tolower()\n\nsection_links <- paste0(\"/browse/style/\", styles, \".html\")\n```\n\n  Now, in one specific page (\"Rock\"), we need to figure out how to get the list of artists. We need to do it since we will need a list of all the artists to scrap the data with the `scrap_artist` function. Let's start by getting the objects with `class=moreNamesContainer` (\".moreNamesContainer\") as identified in the page's code with \\textbf{Selector Gadget}.\n  \n  The table containing all the names for each section page is converted to a list (which unfortunately has 4 levels), so the code below is able to extract the links. It's already inside a function `artist_links` that will be useful for us.\n  \n```{r}\nget_artist_links <- function(section_link) {\n    # Reading the page\n    page <- read_html(paste0(\"https://www.vagalume.com.br\", section_link))\n\n    # Removing desired node as a list\n    nameList <- page %>%\n        html_nodes(\".moreNamesContainer\") %>%\n        as_list()\n\n    # Removing undesired list levels and extracting 'href' attribute\n    # 'NOT ELEGANT AT ALL' ALLERT\n    artist_links <- nameList %>%\n        unlist(recursive = F) %>% # Removing first undesired level\n        unlist(recursive = F) %>% # Removing second undesired level\n        unlist(recursive = F) %>% # Removig third undesired level, ugh\n        map(., ~ attr(., \"href\")) %>% # Mapping through list to extract attrs\n        unlist() %>%\n        as.character() # Removing names and parsing to a vector\n\n    return(unique(artist_links))\n}\n\nget_artist_links(section_link = \"/browse/style/rock.html\") %>% head(10)\n```\n\n  Ouch, what inelegant code, probably due to my lack of knowledge on `purrr`, I definitely need to get deeper than `map_*()` on this package soon. But ok, let's map it through all values of `section_links` in order to obtain the links to all artists in the website. \n  \n  In order to increase performance we will use `furrr::future_map()` which is a function that conveniently combines `purrr::map()` with the parallel processing provided by the `future` package.\n\n```{r}\n# Planning parallel processing\nplan(multisession)\n\n# Getting all artists' links from the website\n#all_artists_links <- get_artist_links(\"/browse/style/sertanejo.html\")\nall_artists_links <- future_map(section_links, ~get_artist_links(.)) %>% \nunlist()\n```\n  \n  I got $3.255$ links when I scrapped the data in February 10th in 2019. That leads us to the first big step in this endeavour, our Artists dataset.\n  \n## Building the dataset\n\n  Now that we have the scrapper function `scrap_artist` and a list to map it through, we can build our dataset. Since our scrapper returns a tibble i will use the `map_dfr` variation which already binds the resulting data frames by rows. Of course there is a `furrr::future_map_dfr()` to save is since this operation is going to be much more demanding than the last one.\n  \n```{r, eval=FALSE}\n# De\np_scrap_artist <- possibly(scrap_artist, \n                           otherwise = tibble(Artist = NA, \n                                              Genres = NA, \n                                              Songs = NA, \n                                              Popularity = NA, \n                                              Link = NA))\n\n# Planning parallel processing\nplan(multisession)\n\n# Getting all artists' links from the website\nall_artists <- future_map_dfr(all_artists_links, ~p_scrap_artist(.))\n```\n\n\n  It took around 16 minutes to finish this data scraping on my gaming laptop, it may depend on internet connection and processing power. Notice that I used `possibly()` to create a new function. It wraps a function and in case it returns an error it won't stop our code, instead it will return what I passed to the `otherwise` argument, which is a tibble of `NA`s. It's possible to use the argument `.progress = TRUE` in the `future_map_*` functions in order to check the map procedure progress in case of big operations like this one.\n  \n  Originally i wanted to maintain only some genres as shown bellow, but on the most recent revision of the data on Kaggle i scraped all the genres, so the code below is commented but you can use it if you want specific genres to be scraped:\n\n```{r, warning=FALSE, eval=FALSE}\n# Selecting genre labels to keep\n#genres_keep <- c(\"Rock\", \"Hip Hop\", \"Pop\", \"Sertanejo\", \"Funk Carioca\", \"Samba\")\n\n\n# Removing other genre labels\n#all_artists_fixed <- all_artists %>% \n#  separate(Genres, c(\"G1\", \"G2\", \"G3\"), sep = \"; \") %>%  # Separating Genres variable\n#  gather(G1:G3, key = \"G\", value = \"Genre\") %>% select(-G) %>% \n#  filter(Genre %in% genres_keep)\n```\n  \n  There, now we have some data on all artists on the Genres I specified. The number of rows grew up to $3622$ because of artists that had more than one label, I will rather maintain it this way since removing the duplicates might bias the results to one genre, at least this way the artists weight equally both genres they are present in. The dataset on artists is built! We can do a last step on the data scraping before we stop for today: Scraping their lyrics!\n\n# 2 Scraping lyrics data\n\n  I decided to already scrap some data on the lyrics to use in the future analysis post, some text analysis, maybe some sentiment analysis and then I have plans for a lyrics-composing AI, I will create a little monster that will ruin the music industry, ok I'll stop dreaming. Let's do it!\n  \n## Building the scrapper\n\n  We are going to do it similarly to how we did it with the artists: do a scrapper to get all the individual links then map throuhg it. I already built a lyrics scraper `get_lyric()`, that returns the lyric based on the songs' link. the code below defines it:\n\n```{r}\n# Extracts a single lyric from a song link\nget_lyric <- function(song_link){\n  \n  # Reading the html page\n  lyric <- read_html(paste0(\"https://www.vagalume.com.br\", song_link)) %>% \n    html_nodes(\"#lyrics\") %>% \n    html_text2()\n  \n  return(lyric)\n}\n\n# Testing it on Holiday from Green Day\nget_lyric(song_link = \"/green-day/holiday.html\")\n```\n  \n  From this specific lyric we can see that there are still some special characters. Instead of just treating this \"\\\\\" issue, I'll remove all special characters when analyzing the lyrics. Better, right? Let's get to the next step on our scraping adventure.\n  \n  \n## Links to scrap the data from\n  \n  Now we need the list of songs to map the scraper through. The code below defines `get_lyrics_links()`, which receives the link to the artist then returns a tibble with name and link to the music. It also returns the link to the artist that was passed, in order to use it as an ID for the artist dataset in the future. \n\n```{r}\n# Extracts all lyrics from an artist link - uses get_lyric()\nget_lyrics_links <- function(artist_link){\n  \n  # Reading the html page on the artist\n  page <- read_html(paste0(\"https://www.vagalume.com.br\", artist_link))\n  \n  # Obtaining all the musics' links -\n  music_name_node <- page %>% html_nodes(\".nameMusic\")\n  music_names <- music_name_node %>% html_text()\n  music_links <- music_name_node %>% html_attr(\"href\")\n  \n  # Building final tibble\n  res <- tibble(ALink = rep(artist_link, length(music_names)),\n                SName = music_names,\n                SLink = music_links)\n  \n  return(res)\n}\n\n# Testing final function\nget_lyrics_links(\"/green-day/\") %>% head(5)\n```\n\n  Notice that I use `html_attr(\"href\")` to extract the attribute \"href\" from the html object, which is the link to the song page. Let's map this function through all the artists we have in order to obtain the links to the musics:\n  \n```{r, eval=FALSE}\n# Generating safe version of get_lyrics_links()\np_get_lyrics_links <- possibly(get_lyrics_links, \n                               otherwise = tibble(ALink = NA,\n                                                  SName = NA))\n\n# Returning all the links to the musics\nplan(multisession)\nall_songs_links <- future_map_dfr(all_artists_links, ~p_get_lyrics_links(.))\n```\n  \n  After a 82.98 seconds wait on my case we found out that there are $209.522$ songs for our $3.355$ artists. \n\n## Building the lyrics dataset  \n  \n  Now all we gotta do is map hour scraper `get_lyric()` through all this songs to get our final lyrics dataset.\n  \n```{r, eval=FALSE}\n# Gerenating a safe version with possibly()\np_get_lyric <- possibly(get_lyric, otherwise = NA)\n\n# Mapping it through the all_artists_links vector\nplan(multisession)\nall_lyrics <- future_map_chr(all_songs_links$SLink, ~p_get_lyric(.)[1], .progress = TRUE)\n\n# Adding it to the dataframe\nall_songs_links$Lyric <- all_lyrics %>% str_replace_all(\"\\\\. \\\\. \", \". \")\n```\n\n  This one took around 9,064.08 seconds (151 minutes) to finish, quite some time! \n  \n  You might be asking why not build a single function to scrap the lyrics. Well, I tried it at first, a function that took the artist link, mapped through all song link inside it and returned a tibble with all the artist lyrics. But it was taking hours to run iterally. I think it happens because `furrr` can't optimize well functions that move great pieces of data around. Also, it was a function with a map call inside, seems like it's not possible to call a future within a future call so I had to separate them in order to optimize running time.\n  \n  Finally, we can use the `cld2` package to infer each lyric's language to make future analysis easier:\n  \n```{r, eval=FALSE}\nlibrary(cld2)\n\nall_songs_links <- all_songs_links %>% \n  mutate(language = detect_language(Lyric))\n```\n\n# 3 That's it!\n\n  Both datasets can be foun on my Kaggle datasets page, here. Feel free to analyze it the way you want!\n\n  This post is the first of two parts of the music analysis. In the next one i plan on briefly analyzing this data and training a LSTM in the lyrics for each genre. I hope you liked it so far! If you have any feedback on your mind, share it in my twitter [\\@a_neisse](https://twitter.com/a_neisse) or on the contact section of this website.\n\n<style>\nbody {\ntext-align: justify}\n</style>"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"include-in-header":[{"text":"<style>\n.quarto-title-block .quarto-title-banner {\n  background-position-y: center;\n  height: 200px;\n}\n</style>\n"}],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":{"light":"cosmo","dark":["cosmo","../../theme-dark.scss"]},"title":"**Scraping lyrics from Vagalume**","description":"Using R to webscrape vagalume.com","author":"Anderson Neisse","date":"2019-10-02","categories":["Webscraping","Music","Code"],"image":"featured.png","title-block-banner":"featured.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}