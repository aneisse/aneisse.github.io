[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anderson Neisse",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Kaggle\n  \n  \n    \n     Github\n  \n\n  \n  \nI am a statistician with a masters in applied statistics and biometrics who loves the idea of figuring out the world using data and the scientific method. I am an enthusiast of the possibilities brought by data combined with awesome tools like programming, statistics, machine learning and artificial intelligence and am always looking at the world searching for awesome opportunities to generate value with.\n\n\nEducationExperience\n\n\nFederal University of Mato Grosso | Viçosa, MG - Brazil  Msc in Applied Statistics and Biometrics | 2018 - 2019\nFederal University of Mato Grosso | Cuiabá, MT - Brazil  BS in Statistics | 2012 - 2017\n\n\nMato Grosso State Court | Jan/2020 - present  Statistician\nMato Grosso State Public Ministry | Sep/2019 - Jan/2020  Statistician\nMalta Dunning Company | Apr/2013 - Jan/2018  Data Analysis and Report"
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Anderson Neisse",
    "section": "Posts",
    "text": "Posts\nCheck all the posts. The most recent ones are:\n\n\n\n\n  \n\n\n\n\nScraping lyrics from Vagalume\n\n\n\n\n\nUsing R to webscrape vagalume.com\n\n\n\n\n\n\nOct 2, 2019\n\n\nAnderson Neisse\n\n\n\n\n\n\n  \n\n\n\n\nR4DS Slack Community data\n\n\n\n\n\nData analysis of signup data from R4DS\n\n\n\n\n\n\nDec 15, 2017\n\n\nAnderson Neisse\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Anderson Neisse",
    "section": "Projects",
    "text": "Projects\nCheck all the projects. The most recent ones are:\n\n\n\n\n  \n\n\n\n\nDataSciMT Community\n\n\nA Data Science discussion community to stimulate the growth of the Data Science in the state of Mato Grosso, Brazil.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSong Lyrics Scraping\n\n\nA project that aimed to scrape song lyrics from the Vagalume brazilian site and make it available in Kaggle.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here you can find some projects for fun that I get myself into every once in a while, you can also check my Kaggle Profile.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nDataSciMT Community\n\n\nA Data Science discussion community to stimulate the growth of the Data Science in the state of Mato Grosso, Brazil.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSong Lyrics Scraping\n\n\nA project that aimed to scrape song lyrics from the Vagalume brazilian site and make it available in Kaggle.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/DataSci/datasci.html",
    "href": "projects/DataSci/datasci.html",
    "title": "DataSciMT Community",
    "section": "",
    "text": "The DataSciMT is an idea that me and my fellow Statistician Rodrigo Oliveira came up with when we realized that our state, Mato Grosso (Brazil), is full of missed opportunities for Data Science to be applied. So we came up with a discussion community aiming to promote meetups and, most recently, lives in YouTube where we discuss the topics and use cases of Data Science and its various applications.\nAt the beginning, in November 2019, we hosted local meetups to promote networking and exchange knowledge and experience inviting Data Science enthusiasts to delve in such a tremendously interesting field. Since it focuses on the development of the local community all contents are primarily in Portuguese (pt-BR).\n\n\nMaterials\n\n\n\n Videos  Github"
  },
  {
    "objectID": "posts/20190222-lyrics-scraping/index.html",
    "href": "posts/20190222-lyrics-scraping/index.html",
    "title": "Scraping lyrics from Vagalume",
    "section": "",
    "text": "Today I woke up with a desire to stretch my web scraping skills and willing to do so while listening to some music, so why not scrap some music data? In this post I will scrap som data on artists and their lyrics so in a future post I plan on having some visualizations on the data as well as train a LSTM on the lyrics and maybe make it compose some new ones!\nThis whole post is about scraping the data from a Brazilian lyrics website called Vagalume (literally Firefly in PT-BR), which stores music lyrics for a lot of artists, not only Brazilians. The site’s evaluations of popularity might result on some cool visualizations. And of course, it’s accessed almost exclusively by brazilians, so it will be informative about their tastes, on some level.\nNOTE: The datasets generated by this posts are stored in Kaggle here, in case you want them to do some analysis.\nSome packages we will need for this task:\nlibrary(tidyverse)\nlibrary(httr)\nlibrary(xml2)\nlibrary(rvest)\nlibrary(furrr)\nThe packages httr, xml2 and rvest are the main ones for html-based web scraping these days. Loading tidyverse for all reasons and furrr for combination of future and purrr packages.\nThe website has a A-to-Z list of artists, with a link for each letter, then a link for each artist with such starting letter, which contains a list of music names with lyrics “inside”. It also displays the artists in music genre sections, which I will use, they work very similarly to the A-to-Z sections. I’ll need to go through lots of links in order to collect all the data I want."
  },
  {
    "objectID": "posts/20190222-lyrics-scraping/index.html#building-the-scrapper",
    "href": "posts/20190222-lyrics-scraping/index.html#building-the-scrapper",
    "title": "Scraping lyrics from Vagalume",
    "section": "Building the scrapper",
    "text": "Building the scrapper\nI’m going to start with scraping the band’s name to illustrate how rvest works with web scraping. The code below does that job for us. The read_html() parses a html page into R. Then, html_nodes() extracts a specific part (node) from the page that contains the data we want. You can define the node’s name with Selector Gadget or by inspecting the page’s code in the browser. Also, a fun and very instructive tutorial on selecting css labels is CSS Dinner. Finally, we turn the lyric into text, removing line breaks and this sort of stuff with html_text.\n\n# Getting an artists name\nread_html(\"https://www.vagalume.com.br/green-day/\") %&gt;%\n    html_nodes(\".darkBG\") %&gt;%\n    html_text()\n\nThere, now we will try to get the labels for music genre for the band:\n\n# Scraping data on genre\nread_html(\"https://www.vagalume.com.br/green-day/\") %&gt;%\n    html_nodes(\".subHeaderTags\") %&gt;%\n    as_list() %&gt;%\n    unlist() %&gt;%\n    paste(collapse = \"; \")\n\nIn this case we need to parse it to a list using xml2::as_list() and then use apply::unlist() in order to transform the html to a vector. If we used rvest::html_text() it would have concatenated all the labels with sep = \"\". Green day has the labels: “Rock”; “Punk Rock” and “Rock Alternativo” (Alternative Rock).\nNow let’s try and get number of musics for the band. The main page for them has the list of all their songs, we can get them and them store their count.\n\nread_html(\"https://www.vagalume.com.br/green-day/\") %&gt;%\n    html_nodes(\".nameMusic\") %&gt;%\n    html_text() %&gt;%\n    unique() %&gt;%\n    length()\n\nThe first line of code above, if ran alone, returns a vector with all the music names in the page. However before the list of all songs there is a top 25 of the artist’s songs, so we remove duplicates by using unique() and then count how many songs are there using length() applied to the resulting vector of unique song names.\nNow, last but not least important, the artist’s popularity. This one is stored in a sub-page (\\popularidade\\) of each artist. In the end of the page it shows the current popularity in a pop which always begins with the word “Atualmente”.\n\nread_html(\"https://www.vagalume.com.br/green-day/popularidade/\") %&gt;%\n    html_nodes(\".text\") %&gt;%\n    html_text()\n\nSo we need to filter the one that begins with that word (which means ‘Currently’ by the way). And then we need to extract the value of popularity (9.0 in this case) which is always in between the words “em” and “pontos”.\n\nread_html(\"https://www.vagalume.com.br/green-day/popularidade/\") %&gt;%\n    html_nodes(\".text\") %&gt;%\n    html_text() %&gt;%\n    # Extracting last phrase\n    tail(1) %&gt;%\n    # Using Regular Expressions to remove the number\n    str_extract(., \"(?&lt;=está em )(.*)(?= pontos)\") %&gt;%\n    # Replacing brazilian decimal \",\" by \".\" and arsing to numeric\n    str_replace(\",\", \".\") %&gt;%\n    as.numeric()\n\nIt took some work but we got the popularity for the band, shame it’s rounded up to an integer, but it won’t be completely useless.\nNow we know how to get every information we need from a single artist, so let’s build our scraper function:\n\nscrap_artist &lt;- function(artist_link) {\n    # Reading the entire pages\n    page &lt;- read_html(paste0(\"https://www.vagalume.com.br\", artist_link))\n    pop_page &lt;- read_html(paste0(\"https://www.vagalume.com.br\", artist_link, \"popularidade/\"))\n\n    # Getting desired variables\n    A &lt;- page %&gt;%\n        html_nodes(\".darkBG\") %&gt;%\n        html_text()\n\n    G &lt;- page %&gt;%\n        html_nodes(\".subHeaderTags\") %&gt;%\n        as_list() %&gt;%\n        unlist() %&gt;%\n        paste(collapse = \"; \")\n\n    S &lt;- page %&gt;%\n        html_nodes(\".nameMusic\") %&gt;%\n        html_text() %&gt;%\n        unique() %&gt;%\n        length()\n\n    P &lt;- pop_page %&gt;%\n        html_nodes(\".text\") %&gt;%\n        html_text() %&gt;%\n        tail(1) %&gt;%\n        str_extract(., \"(?&lt;=está em )(.*)(?= pontos)\") %&gt;%\n        str_replace(\",\", \".\") %&gt;%\n        as.numeric()\n\n    # Creating tibble\n    res &lt;- tibble(\n        Artist = A,\n        Genres = G,\n        Songs = S,\n        Popularity = P,\n        Link = artist_link\n    )\n    return(res)\n}\n\n# Testing the scrapper function\nscrap_artist(\"/green-day/\")\n\nNice, a single function that receives a link to an artist and returns all the data we defined for that artist. Notice that I defined a function such as it receives only the important part of the link, which is what we will scrap from the site in order to form our data frame."
  },
  {
    "objectID": "posts/20190222-lyrics-scraping/index.html#obtaining-links-of-artists",
    "href": "posts/20190222-lyrics-scraping/index.html#obtaining-links-of-artists",
    "title": "Scraping lyrics from Vagalume",
    "section": "Obtaining links of artists",
    "text": "Obtaining links of artists\nNow, since this post is just to stretch my web scraping skills and have some fun I won’t scrap data from all the artists, which could take ages as the process depends not only on the processing power but also on the internet connection and time to get to each link. However, we will get some music genres to play:\n\nRock\nHip Hop\nPop music\nSertanejo (Basically the Brazilian version of Country Music)\nFunk Carioca (Originated 60s US Funk, a completely different genre in Brazil nowadays)\nSamba (Typical Brazilian music)\n\nIn order to map our scrapper to artists we need to have the link to their pages in the website. For that we need to get the links for music genre sections we want to scrap:\n\npage &lt;- read_html(\"https://www.vagalume.com.br/browse/style/\")\n\nstyles &lt;- page %&gt;%\n    html_nodes(\".itemTitle\") %&gt;%\n    html_text() %&gt;%\n    str_replace(., \"/\", \"-\") %&gt;%\n    str_replace(., \"á\", \"a\") %&gt;%\n    str_replace(., \"é\", \"e\") %&gt;%\n    str_replace(., \"â\", \"a\") %&gt;%\n    str_replace(., \"ó\", \"o\") %&gt;%\n    str_replace_all(., \"ú\", \"u\") %&gt;%\n    str_replace(., \"&\", \"-n-\") %&gt;%\n    str_replace(., \" \", \"-\") %&gt;%\n    str_replace(., \"J-Pop-J-Rock\", \"j-pop\") %&gt;%\n    str_replace(., \"Gospel-Religioso\", \"gospel\") %&gt;%\n    str_replace(., \"Romantico\", \"romantica\") %&gt;%\n    tolower()\n\nsection_links &lt;- paste0(\"/browse/style/\", styles, \".html\")\n\nNow, in one specific page (“Rock”), we need to figure out how to get the list of artists. We need to do it since we will need a list of all the artists to scrap the data with the scrap_artist function. Let’s start by getting the objects with class=moreNamesContainer (“.moreNamesContainer”) as identified in the page’s code with .\nThe table containing all the names for each section page is converted to a list (which unfortunately has 4 levels), so the code below is able to extract the links. It’s already inside a function artist_links that will be useful for us.\n\nget_artist_links &lt;- function(section_link) {\n    # Reading the page\n    page &lt;- read_html(paste0(\"https://www.vagalume.com.br\", section_link))\n\n    # Removing desired node as a list\n    nameList &lt;- page %&gt;%\n        html_nodes(\".moreNamesContainer\") %&gt;%\n        as_list()\n\n    # Removing undesired list levels and extracting 'href' attribute\n    # 'NOT ELEGANT AT ALL' ALLERT\n    artist_links &lt;- nameList %&gt;%\n        unlist(recursive = F) %&gt;% # Removing first undesired level\n        unlist(recursive = F) %&gt;% # Removing second undesired level\n        unlist(recursive = F) %&gt;% # Removig third undesired level, ugh\n        map(., ~ attr(., \"href\")) %&gt;% # Mapping through list to extract attrs\n        unlist() %&gt;%\n        as.character() # Removing names and parsing to a vector\n\n    return(unique(artist_links))\n}\n\nget_artist_links(section_link = \"/browse/style/rock.html\") %&gt;% head(10)\n\nOuch, what inelegant code, probably due to my lack of knowledge on purrr, I definitely need to get deeper than map_*() on this package soon. But ok, let’s map it through all values of section_links in order to obtain the links to all artists in the website.\nIn order to increase performance we will use furrr::future_map() which is a function that conveniently combines purrr::map() with the parallel processing provided by the future package.\n\n# Planning parallel processing\nplan(multisession)\n\n# Getting all artists' links from the website\n#all_artists_links &lt;- get_artist_links(\"/browse/style/sertanejo.html\")\nall_artists_links &lt;- future_map(section_links, ~get_artist_links(.)) %&gt;% \nunlist()\n\nI got \\(3.255\\) links when I scrapped the data in February 10th in 2019. That leads us to the first big step in this endeavour, our Artists dataset."
  },
  {
    "objectID": "posts/20190222-lyrics-scraping/index.html#building-the-dataset",
    "href": "posts/20190222-lyrics-scraping/index.html#building-the-dataset",
    "title": "Scraping lyrics from Vagalume",
    "section": "Building the dataset",
    "text": "Building the dataset\nNow that we have the scrapper function scrap_artist and a list to map it through, we can build our dataset. Since our scrapper returns a tibble i will use the map_dfr variation which already binds the resulting data frames by rows. Of course there is a furrr::future_map_dfr() to save is since this operation is going to be much more demanding than the last one.\n\n# De\np_scrap_artist &lt;- possibly(scrap_artist, \n                           otherwise = tibble(Artist = NA, \n                                              Genres = NA, \n                                              Songs = NA, \n                                              Popularity = NA, \n                                              Link = NA))\n\n# Planning parallel processing\nplan(multisession)\n\n# Getting all artists' links from the website\nall_artists &lt;- future_map_dfr(all_artists_links, ~p_scrap_artist(.))\n\nIt took around 16 minutes to finish this data scraping on my gaming laptop, it may depend on internet connection and processing power. Notice that I used possibly() to create a new function. It wraps a function and in case it returns an error it won’t stop our code, instead it will return what I passed to the otherwise argument, which is a tibble of NAs. It’s possible to use the argument .progress = TRUE in the future_map_* functions in order to check the map procedure progress in case of big operations like this one.\nOriginally i wanted to maintain only some genres as shown bellow, but on the most recent revision of the data on Kaggle i scraped all the genres, so the code below is commented but you can use it if you want specific genres to be scraped:\n\n# Selecting genre labels to keep\n#genres_keep &lt;- c(\"Rock\", \"Hip Hop\", \"Pop\", \"Sertanejo\", \"Funk Carioca\", \"Samba\")\n\n\n# Removing other genre labels\n#all_artists_fixed &lt;- all_artists %&gt;% \n#  separate(Genres, c(\"G1\", \"G2\", \"G3\"), sep = \"; \") %&gt;%  # Separating Genres variable\n#  gather(G1:G3, key = \"G\", value = \"Genre\") %&gt;% select(-G) %&gt;% \n#  filter(Genre %in% genres_keep)\n\nThere, now we have some data on all artists on the Genres I specified. The number of rows grew up to \\(3622\\) because of artists that had more than one label, I will rather maintain it this way since removing the duplicates might bias the results to one genre, at least this way the artists weight equally both genres they are present in. The dataset on artists is built! We can do a last step on the data scraping before we stop for today: Scraping their lyrics!"
  },
  {
    "objectID": "posts/20190222-lyrics-scraping/index.html#building-the-scrapper-1",
    "href": "posts/20190222-lyrics-scraping/index.html#building-the-scrapper-1",
    "title": "Scraping lyrics from Vagalume",
    "section": "Building the scrapper",
    "text": "Building the scrapper\nWe are going to do it similarly to how we did it with the artists: do a scrapper to get all the individual links then map throuhg it. I already built a lyrics scraper get_lyric(), that returns the lyric based on the songs’ link. the code below defines it:\n\n# Extracts a single lyric from a song link\nget_lyric &lt;- function(song_link){\n  \n  # Reading the html page\n  lyric &lt;- read_html(paste0(\"https://www.vagalume.com.br\", song_link)) %&gt;% \n    html_nodes(\"#lyrics\") %&gt;% \n    html_text2()\n  \n  return(lyric)\n}\n\n# Testing it on Holiday from Green Day\nget_lyric(song_link = \"/green-day/holiday.html\")\n\nFrom this specific lyric we can see that there are still some special characters. Instead of just treating this “\\” issue, I’ll remove all special characters when analyzing the lyrics. Better, right? Let’s get to the next step on our scraping adventure."
  },
  {
    "objectID": "posts/20190222-lyrics-scraping/index.html#links-to-scrap-the-data-from",
    "href": "posts/20190222-lyrics-scraping/index.html#links-to-scrap-the-data-from",
    "title": "Scraping lyrics from Vagalume",
    "section": "Links to scrap the data from",
    "text": "Links to scrap the data from\nNow we need the list of songs to map the scraper through. The code below defines get_lyrics_links(), which receives the link to the artist then returns a tibble with name and link to the music. It also returns the link to the artist that was passed, in order to use it as an ID for the artist dataset in the future.\n\n# Extracts all lyrics from an artist link - uses get_lyric()\nget_lyrics_links &lt;- function(artist_link){\n  \n  # Reading the html page on the artist\n  page &lt;- read_html(paste0(\"https://www.vagalume.com.br\", artist_link))\n  \n  # Obtaining all the musics' links -\n  music_name_node &lt;- page %&gt;% html_nodes(\".nameMusic\")\n  music_names &lt;- music_name_node %&gt;% html_text()\n  music_links &lt;- music_name_node %&gt;% html_attr(\"href\")\n  \n  # Building final tibble\n  res &lt;- tibble(ALink = rep(artist_link, length(music_names)),\n                SName = music_names,\n                SLink = music_links)\n  \n  return(res)\n}\n\n# Testing final function\nget_lyrics_links(\"/green-day/\") %&gt;% head(5)\n\nNotice that I use html_attr(\"href\") to extract the attribute “href” from the html object, which is the link to the song page. Let’s map this function through all the artists we have in order to obtain the links to the musics:\n\n# Generating safe version of get_lyrics_links()\np_get_lyrics_links &lt;- possibly(get_lyrics_links, \n                               otherwise = tibble(ALink = NA,\n                                                  SName = NA))\n\n# Returning all the links to the musics\nplan(multisession)\nall_songs_links &lt;- future_map_dfr(all_artists_links, ~p_get_lyrics_links(.))\n\nAfter a 82.98 seconds wait on my case we found out that there are \\(209.522\\) songs for our \\(3.355\\) artists."
  },
  {
    "objectID": "posts/20190222-lyrics-scraping/index.html#building-the-lyrics-dataset",
    "href": "posts/20190222-lyrics-scraping/index.html#building-the-lyrics-dataset",
    "title": "Scraping lyrics from Vagalume",
    "section": "Building the lyrics dataset",
    "text": "Building the lyrics dataset\nNow all we gotta do is map hour scraper get_lyric() through all this songs to get our final lyrics dataset.\n\n# Gerenating a safe version with possibly()\np_get_lyric &lt;- possibly(get_lyric, otherwise = NA)\n\n# Mapping it through the all_artists_links vector\nplan(multisession)\nall_lyrics &lt;- future_map_chr(all_songs_links$SLink, ~p_get_lyric(.)[1], .progress = TRUE)\n\n# Adding it to the dataframe\nall_songs_links$Lyric &lt;- all_lyrics %&gt;% str_replace_all(\"\\\\. \\\\. \", \". \")\n\nThis one took around 9,064.08 seconds (151 minutes) to finish, quite some time!\nYou might be asking why not build a single function to scrap the lyrics. Well, I tried it at first, a function that took the artist link, mapped through all song link inside it and returned a tibble with all the artist lyrics. But it was taking hours to run iterally. I think it happens because furrr can’t optimize well functions that move great pieces of data around. Also, it was a function with a map call inside, seems like it’s not possible to call a future within a future call so I had to separate them in order to optimize running time.\nFinally, we can use the cld2 package to infer each lyric’s language to make future analysis easier:\n\nlibrary(cld2)\n\nall_songs_links &lt;- all_songs_links %&gt;% \n  mutate(language = detect_language(Lyric))"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers",
    "section": "",
    "text": "Check out my published papers in this section. You could also check them and its citations at my Google Scholar Profile.\n\n2022\n\nInventário de ninhos de abelhas sociais sem ferrão em uma área antropizada. JM Albernaz, CAL de Carvalho, FL Silva, AC Neisse, IP Silva, MAPC Costa, CS Machado, GS Sodré - Diversitas Journal https://diversitas.emnuvens.com.br/diversitas_journal/article/view/1810\n\n\n\n2021\n\nMachine learning algorithms to predict the dry matter loss of stored soybean grains (Glycine max L.). JS Zeymer, F Guzzo, MEV de Araujo, RS Gates, PC Correa, MCTR Vidigal, AC Neisse - Journal of Food Process Engineering https://onlinelibrary.wiley.com/doi/abs/10.1111/jfpe.13820\n\n\n\n2021\n\nChronic fatigue syndrome and its relation with absenteeism: elastic-net and stepwise applied to biochemical and anthropometric clinical measurements. AC Neisse, FLP Oliveira, ACS Oliveira, FRB Cruz, RM Nascimento Neto - Brazilian Journal of Biometrics http://www.biometria.ufla.br/index.php/BBJ/article/view/533/298\n\n\n\n2020\n\nAnálise de sobrevivência aplicada ao trancamento de matrícula no curso de graduação em estatística de uma universidade federal. JL Kirch, AC Neisse, TCMA Veloso - E&S Engineering and Science https://periodicoscientificos.ufmt.br/ojs/index.php/eng/article/view/7030\n\n\n\n2018\n\nAMMI and GGE Biplot for genotype× environment interaction: a medoid–based hierarchical cluster analysis approach for high–dimensional data. AC Neisse, JL Kirch, K Hongyu - Biometrical Letters https://sciendo.com/article/10.2478/bile-2018-0008\n\n\n\n2017\n\nVariáveis Psicológicas e Desempenho Acadêmico: Uma Análise da Existência de Correlação Canônica. AC Neisse, K Hongyu - E&S Engineering and Science. https://periodicoscientificos.ufmt.br/ojs/index.php/eng/article/view/4577\n\n\n\n2016\n\nAplicação de componentes principais e análise fatorial a dados criminais de 26 estados dos EUA. AC Neisse, K Hongyu - E&S Engineering and Science. https://periodicoscientificos.ufmt.br/ojs/index.php/eng/article/view/4354"
  },
  {
    "objectID": "posts/20180102-r4ds-slack-group/index.html",
    "href": "posts/20180102-r4ds-slack-group/index.html",
    "title": "R4DS Slack Community data",
    "section": "",
    "text": "Before anything else, I tried HARD to make tabsets work with blogdown but it doesn’t offer support to tabsets at all, nor folded code chunks, sadly. That said, this post might look longer than it really is! Unless you want to read every chunk of code, then it indeed IS as long as it seems."
  },
  {
    "objectID": "posts/20180102-r4ds-slack-group/index.html#basic-distributions",
    "href": "posts/20180102-r4ds-slack-group/index.html#basic-distributions",
    "title": "R4DS Slack Community data",
    "section": "3.1 Basic distributions",
    "text": "3.1 Basic distributions\nFrom now on, if you’re completely new to the tidyverse or to R itself, things might get shadier in terms of code, but I promise the visualizations will still be worth!\nThere are some variables that are not obligatory in the registration form for the R4DS Online Learning Community. That said, we shoul firstly have a look at NA values in our dataset.\n\n#Counting NA cases in each column\nmap_dbl(dados, ~sum(is.na(.) | . == \"\"))\n\n    Type  RStudio    Stats Location   Cohort      lon      lat \n       0        0        0        0        0      122      122 \n\n#Removing rows with only NA cases\ndados &lt;- dados %&gt;% filter(!is.na(Type))\n\nWell, looks like there are 3 rows with no data at all and other 4 rows with no location. I removed rows with NA values in every column (no data at all). I didn’t remove the rows with no location because they’ll still contribute with other information and we can remove them right before using the Location column.\nNow that we’ve already cleaned the dataset from useless rows let’s have a look at each variable separately and work from that! The plot tabs below show frequency distributions for each variable. It is worth to note that each tab in the tabset will display a different code, just as it displays a different graph.\n##{.tabset .tabset-fade .tabset-pills}\n\nCohorts\n\n#Transforming the data and generating the plot\nplot &lt;- dados %&gt;% \n  count(Cohort) %&gt;%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %&gt;%\n  ggplot(aes(Cohort, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by Cohort\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n\n\n\n\n\n\n\n\n\nTypes\n\n#Transforming the data and generating the plot\nplot &lt;- dados %&gt;% \n  count(Type) %&gt;%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %&gt;%\n  ggplot(aes(Type, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by Type\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n\n\n\n\n\n\n\n\n\nRStudio\n\n#Transforming the data and generating the plot\nplot &lt;- dados %&gt;% \n  count(RStudio) %&gt;%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %&gt;%\n  ggplot(aes(RStudio, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by confortability with RStudio\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n\n\n\n\n\n\n\n\n\nStats\n\n#Transforming the data and generating the plot\nplot &lt;- dados %&gt;% \n  count(Stats) %&gt;%\n  mutate(Count = n, Proportion = round(n/sum(n)*100, 2)) %&gt;%\n  ggplot(aes(Stats, Count, labels = Proportion)) +\n  geom_bar(fill = \"dodgerblue2\", stat = \"identity\") +\n  labs(title = \"Members' frequency by confortability with Statistics and Analysis\", y = \"Frequency\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")"
  },
  {
    "objectID": "posts/20180102-r4ds-slack-group/index.html#section",
    "href": "posts/20180102-r4ds-slack-group/index.html#section",
    "title": "R4DS Slack Community data",
    "section": "",
    "text": "It is worth to note that these plots concern people who joined up, not necessarily active people. That said, the first plot shows us the frequency of each invitation (Cohort). We can also see at the Type frequency that there is a 3.8 Learner/Mentor ratio.\nAs for RStudio, it has mean 2.9 with the frequency distribution rather stable despite not very higher frequencies around 3. Such behavior is stronger in Stats which has steeper increases as we approach the scale 3 from each side with a mean of 3.2. We could say that our average member is a 3 in the comfortability both in terms of RStudio and Stats, however, in terms of RStudio there is more variability around 3 than Stats.\nBut before any thoughts about this info we should look deeper into the data, seeing if these frequencies change based on any variable’s interaction."
  },
  {
    "objectID": "posts/20180102-r4ds-slack-group/index.html#vizualizing-relations",
    "href": "posts/20180102-r4ds-slack-group/index.html#vizualizing-relations",
    "title": "R4DS Slack Community data",
    "section": "3.2 Vizualizing Relations",
    "text": "3.2 Vizualizing Relations\nWe’ve seen some basic frequency distribution, but are RStudio’s and Stats’ frequency distributions equal when we plot them separately for each Type?"
  },
  {
    "objectID": "posts/20180102-r4ds-slack-group/index.html#section-1",
    "href": "posts/20180102-r4ds-slack-group/index.html#section-1",
    "title": "R4DS Slack Community data",
    "section": "",
    "text": "RStudio by Type\n\n#Transforming the data and generating the plot\nplot &lt;- dados %&gt;%\n  count(RStudio, Type) %&gt;%\n  group_by(Type) %&gt;%\n  mutate(Count = n, Percentage = round(n/sum(n), 4)*100) %&gt;%\n  group_by() %&gt;%\n  ggplot(aes(RStudio, Percentage)) +\n  geom_bar(aes(labels = Count), fill = \"dodgerblue\", stat = \"identity\")+\n  facet_wrap(~Type) +\n  labs(title = \"RStudio scales frequency by Type\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n\n\n\n\n\n\n\n\n\nStats by Type\n\n#Transforming the data and generating the plot\nplot &lt;- dados %&gt;%\n  count(Stats, Type) %&gt;%\n  group_by(Type) %&gt;%\n  mutate(Count = n, Percentage = round(n/sum(n), 4)*100) %&gt;%\n  group_by() %&gt;%\n  ggplot(aes(Stats, Percentage)) +\n  geom_bar(aes(labels = Count), fill = \"dodgerblue\", stat = \"identity\")+\n  facet_wrap(~Type) +\n  labs(title = \"Stats scales frequency by Type\")\n\n#Ploting the interactive graph centralized\ndiv(ggplotly(plot), align = \"center\")\n\n\n\n\n\n\n\n\n\nRStudio vs Stats by Type\n\n#Transforming the data and generating the plot\nplot &lt;- dados %&gt;%\n  count(Type, RStudio, Stats) %&gt;%\n  group_by(Type) %&gt;%\n  mutate(Count = n, Percentage = round(n / sum(n), 4)*100) %&gt;%\n  group_by() %&gt;%\n  ggplot(aes(RStudio, Stats)) +\n  geom_tile(aes(fill = Percentage, labels = Count))+\n  facet_wrap(~Type) +\n  scale_fill_viridis_c()\n\ndiv(ggplotly(plot), align = \"center\")"
  },
  {
    "objectID": "posts/20180102-r4ds-slack-group/index.html#section-2",
    "href": "posts/20180102-r4ds-slack-group/index.html#section-2",
    "title": "R4DS Slack Community data",
    "section": "",
    "text": "The first two plots show us that there really are differences in frequency distributions when we look at each Type separately. Mentors tend to choose higher scales of comfortability both in RStudio and Stats.\nHowever, it gets better when we look at the third graph, _Learners__ are quite frequent when the scales from 1 to 3. But there are a lot of configurations with people highly comfortable in working with RStudio, Stats or both! But now have a look at the Mentors’ plot: there are no Mentors who chose scales lower than 3 in BOTH RStudio and Statistics. And when there IS a 2, the other is a 4.\nWell, interestingly expected pattern, right? I mean, usually, someone who takes the position of Mentor does it as a result of comfortability working with at least one: RStudio and Stats. As it is possible to see in the Mentor’s plot, most of them are pretty comfortable with both RStudio and Stats.\nI would guess that most Mentors with lower scales chose this position for the challenge of improvement by helping others like I did!\nTo explore this data and perceive these patterns is cool, but the thing is: at the end of the day, everybody always help others to find a solution and to get better, no matter if it is a Mentor or a Learner. That why the R4DS Online Learning community is awesome!\nWhat about NOW we see all these awesome members scattered on a map?"
  },
  {
    "objectID": "posts/20180102-r4ds-slack-group/index.html#members-world-map",
    "href": "posts/20180102-r4ds-slack-group/index.html#members-world-map",
    "title": "R4DS Slack Community data",
    "section": "3.2 Members’ World Map",
    "text": "3.2 Members’ World Map\nThe best world maps are the interactive ones! We’re going to need some coordinates so we can plot the map, right? But first, do you remember those Location rows with NA values? We’re going to remove them now:\n\n#Remembering how many of them are there\nmap_dbl(dados, ~sum(is.na(.)))\n\n    Type  RStudio    Stats Location   Cohort      lon      lat \n       0        0        0        0        0      122      122 \n\n#Removing rows with NA in Location\nmp_dt &lt;- filter(dados, !is.na(Location))\n\nAnd there they go, they served their purpose and now are going to Valhalla. I assigned the cleaned data to the mp_dt variable, which stands for “maps data”.\nNow, the ggmap package provides us with the geocode() function, which receives a vector of locations, try to find them in Google Map’s database and returns the best guesses’ coordinates. We’re going to pass Location to the function:\n\n#Passing locations to be searched in Google Maps\nlc_dt &lt;- geocode(tolower(mp_dt$Location), source = \"google\")\n\nmp_dt &lt;- cbind(mp_dt, lc_dt)\n\nIt is worth to note that ggmaps::geocode() doesn’t always return coordinates for all the locations. It happens for 2 reasons: (I) Since the locations are user-typed data, there might be incomplete data keeping geocode() from finding a fair enough answer; (II) At some times google will block some API calls returning “OVER_QUERY_LIMIT”.\nAfter searching for a while about the (II) issue, I found out that it happens because of an intern query limit for the Google API’s Calls. This issue might be solved for some locations by running the code and storing the non-error results in a separate variable and then repeating the process to try and obtain some other locations. Like the code below.\n\n#Run these 2 lines once\nlongitude &lt;- vector(\"double\", length(mp_dt$Location))\nlatitude &lt;- vector(\"double\", length(mp_dt$Location))\n\n#Run these 3 lines as much as yout think is necessary for the result you want\nlc_dt &lt;- geocode(tolower(mp_dt$Location), source = \"google\")\nlongitude &lt;- ifelse(is.na(lc_dt$lon), longitude, lc_dt$lon)\nlatitude &lt;- ifelse(is.na(lc_dt$lat), latitude, lc_dt$lat)\n\n#Check how many NAs are still there\nsum(is.na(longitude)) #Count\nmean(is.na(longitude)) #Percentage\n\n#If you're satisfied with the results, bind the coordinates with the data and go on\nmp_dt &lt;- cbind(mp_dt, longitude, latitude)\n\nI didn’t wrap the code in a while() loop to prevent infinite loops in case the errors never get to 0%, as it could happen in the (I) case.\nIn one of my recent Kernels as a newbie in Kaggle ( Terrorism Worldwide - Exploratory Analysis ) I wrote a not-that-good function to plot choropleth maps using the leaflet package. I’m going to use that function’s code and modify it a little in a way that it plots our members with labelled markers instead of countries’ polygons.\nIn order to plot a map using R, we need data concerning the map itself. The maps::map() function provides us with that.\n\n#Obtaining world map polygons\nworld &lt;- maps::map(\"world\", fill = T, plot = F)\n\n#Creating a leaflet basic map\nm &lt;- leaflet(world) %&gt;% addTiles()\n\nI used the world data to generate the basic layer with leaflet() and addTiles() from the leaflet package.\nAlso, since we’re intended to plot markers for each member, we could use different colors for Mentor and Learners to see it on the map.\n\n#Creating categorical color palette\npal &lt;- colorFactor( RColorBrewer::brewer.pal(2, \"Dark2\"), \n                   domain = mp_dt$Type, na.color = \"white\")\n\nWe’re going to generate some variable strings to be plotted in the map with the markers, these strings are generated with sprintf() and strings based on the C language.\n\n#Generating texts\nstrings &lt;- sprintf(paste(\"&lt;strong&gt;%s&lt;/strong&gt;&lt;br/&gt;&lt;strong&gt;%s&lt;/strong&gt;&lt;br/&gt;\", \n                         names(mp_dt[2]), \": %s&lt;br/&gt;\",\n                         names(mp_dt[3]), \": %s&lt;br/&gt;\",\n                         names(mp_dt[5]), \": %s\"),\n                   mp_dt[[4]], mp_dt[[1]], mp_dt[[2]], mp_dt[[3]], mp_dt[[5]])\n\n#Converting it to html format\nlabels &lt;- strings %&gt;% lapply(htmltools::HTML)\n\nNow we only have to assemble all the parts in the base layer to have our map! I used leaflet::addCircleMarkers() to represent each member with a circle filled with colors based on Type. For guidance with the colors I added a legend with leaflet::addLegend(), and now our interactive map is complete!\n\n#Adding polygon with the variable\nm %&gt;% \n  addCircleMarkers(lng = mp_dt$lon, \n                      lat = mp_dt$lat, \n                      label = ~labels, radius = 1, \n                      color = pal(mp_dt$Type), opacity = 0.75) %&gt;%\n  addLegend(\"bottomright\", pal = pal, values = ~mp_dt$Type,\n  title = names(mp_dt)[1],\n  opacity = 10)\n\n\n\n\n\nEvery circle on the map is identified by all the varibales in the dataset! There are a lot of members on North America and Europe! South America and India come right after in the “rank”."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Check my posts discussing some topics of my interest. You can also check my Kaggle Profile.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nScraping lyrics from Vagalume\n\n\nUsing R to webscrape vagalume.com\n\n\n\nOct 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR4DS Slack Community data\n\n\nData analysis of signup data from R4DS\n\n\n\nDec 15, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Lyrics Scraping/lyrics-scraping.html",
    "href": "projects/Lyrics Scraping/lyrics-scraping.html",
    "title": "Song Lyrics Scraping",
    "section": "",
    "text": "This project aimed, above all, to stretch my web scraping skills while having some fun. The final result was a dataset of song lyrics from 6 musical genres scraped from the Vagalume. The dataset can be used for analyses at Kaggle. I also detailed how i obtained the data (with code in R) in this post.\n\n\nMaterials\n\n\n\n Dataset  Post"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "This page serves as a centralized hub for my most pertinent talks, providing easy access to both the talks themselves and the accompanying slides.\n\n\n\n\nDate\nTitle\nSlides\nSummary\n\n\n\n\nmay/2023\nDa análise exploratória ao aprendizado de máquina: Estatísticos na área de dados\n🖥️\nA talk made during the 8th Statistics Week of the Federal University of Mato Grosso. I talked a little about my own career and some topics i judged important for statisticians planning to enter the Data Science job market at the time.\n\n\njun/2022\nEstatística no Tribunal de Justiça do Estado de Mato Grosso\n🖥️\nA talk made during the 7th Statistics Week of the Federal University of Mato Grosso, presenting the data ecosystem present at the Mato Grosso State Court for the statisticians and other data professionals who work there to navigate on and generate value.\n\n\nMultiple\nR Markdown: Introdução, aplicações e extensões\n🖥️\nA R Markdown tutorial for people who never had any contact with it before hand, covering a wide range of use cases.\n\n\njul/2019\nChronic Fatigue Syndrome: Relations with absenteeism\n🖥️\nTalk on the 64th RBras, on Cuiabá-MT, Brazil, on the topic of Chronic Fatigue Syndrome and its relations with absenteeism, assessed with Elastic-net and Stepwise methods.\n\n\njun/2018\nReamostragem Bootstrap: Introdução, variações e implementação com processamento paralelo\n🖥️\nA seminar on the various Bootstrap methods and its implementations in R using parallele processing for performance gains."
  }
]